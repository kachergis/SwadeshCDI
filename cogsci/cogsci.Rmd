---
title: "Measuring Children's Early Vocabulary Across Languages Using a 'Swadesh' Word List"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf George Kachergis (kachergis@stanford.edu)} 
    \AND {\large \bf Alvin Wei Ming Tan (tanawm@stanford.edu)}
    \AND {\large \bf Virginia A. Marchman (marchman@stanford.edu)} 
    \AND {\large \bf Michael C. Frank (mcfrank@stanford.edu)} \\ Department of Psychology, 450 Jane Stanford Way \\ Stanford, CA 94305 USA}

abstract: >
    Early language skill is predictive of later life outcomes, and is thus of great interest to
    developmental psychologists and clinicians. The Communicative Development Inventories (CDI),
    including a parent-reported inventory of early-learned vocabulary items, have proven to be
    valid and reliable instruments for measuring children's early language skill. CDIs have been
    painstakingly adapted to dozens of languages, and cross-linguistic comparisons thus far show both 
    consistency and variability in language acquisition trajectories. However, hundreds of languages do 
    not yet have CDIs, posing a significant barrier to increasing the diversity of languages that are 
    studied. Here, we propose a method for selecting candidate words to include on new CDIs, leveraging
    analysis of psychometric properties of translation-equivalent concepts that are frequently included on 
    existing CDIs. Leveraging 26 datasets from existing CDIs, we propose a list of 163 concepts that have
    low variability in their cross-linguistic learning difficulty. This pool of 'universal' concepts--
    analogous to the "Swadesh" lists used in glottochronology--can be used as a starting point for future 
    CDI adaptations. We test how well the proposed list generalizes to data from 8 additional languages.
    
keywords: >
    early language learning; CDI; psychometrics; cross-linguistic comparison; Swadesh vocabulary;
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(xtable)
require(mirt)
require(tidyverse)
require(ggpubr)
require(tidyboot)
require(here)

source(here("cross-ling-comparison-helpers.R"))
```

# Introduction

Tools that enable valid assessments of children’s early language abilities are invaluable for researchers, clinicians and parents, as early language skill is predictive of educational outcomes years later [e.g., @bleses2016]. 
The MacArthur-Bates Communicative Development Inventories [CDIs; @Fenson2007; @marchman2023] are parent report assessments that provide reliable and valid estimates of children's early vocabulary size and other aspects of early communicative development, such as gesture use and early use of word combinations. 
Parent report is a relatively quick and low-cost method to assess early language skills since it takes advantage of the fact that parents are "natural observers" of their child's skills and does not depend on a child engaging with an (unfamiliar) experimenter.

Over the years, the CDIs have been adapted to dozens of languages, with forms now available in English, Spanish, French, Hebrew, and Mandarin, to name just a few.
Recently, data from more than 85,000 CDIs in 38 languages have been archived in a central repository [Wordbank; @frank2017]. 
These data have revealed both cross-linguistic consistency and variability in early language skills, with insights from these patterns informing theories of early language learning [@frank2021]. 
For example, cross-linguistic analyses indicate that measures of vocabulary size are tightly correlated with other aspects of early language skill, like gesture and grammatical competence. 
Thus, over development, the language system is "tightly woven" [@bates1994; @frank2021] and early vocabulary size serves as a good proxy measure of children's overall language skill.

On the CDIs, vocabulary size is assessed via a checklist format, which enables caregivers to scan and recognize words their child produces or understands, rather than relying on recall alone. 
For example, the American English CDI Words & Sentences (CDI:WS) form, targeting children 16-30 months of age, is comprised of 680 words from 22 semantic categories, including nouns (e.g., Body Parts, Toys, and Clothing), action words (e.g. verbs), descriptive words (e.g. adjectives), and closed-class words such as pronouns. 
The vocabulary checklist from the American English CDI Words & Gestures (CDI:WG) form, targeting younger children ages 8 to 18 months, is comprised of ~400 words from a similar set of categories. 
Items on these original forms were chosen to reflect a range of difficulty levels (i.e., both easy, moderate, and more difficult), as well as capture the linguistic and societal contexts of (most) children living in the US.
Short versions of each of these forms are also available, each with about 100 items [e.g., @Fenson2000], consisting of a set of items that generate scores that more strongly correlate with scores on the long forms, while retaining representation across a broad set of semantic categories.

<!-- gap! -->
Creating a new CDI requires a lot of effort and resources, presenting a daunting barrier to increasing the diversity of languages studied.
Following the guidelines[^1] from the MacArthur-Bates CDI Advisory Board, the process of adapting a CDI for a language other than American English goes well beyond simply translating items on these forms to that new language. 
While the process can begin with identifying translation equivalents (i.e., items that capture the same general concept in both languages, e.g., "dog" in English, and "perro" in Spanish), the final item set must then be filtered so that all items appropriately reflect the linguistic and sociocultural context of the children learning that language. 
This process usually requires considerable time and effort by researchers who are native speakers of a language, to first select and identify translation equivalents and to then iteratively add, refine, and pilot the new CDI in the target language. 
Because the goal is to obtain the set of items that best capture general trends and individual differences in that language, the items across CDIs in different languages do not necessarily overlap to a great extent. 
For example, the American English CDI:WS and Mexican Spanish CDI:WS forms – two of the first CDIs created – each have 680 words, but only have 463 overlapping concepts (68%).

[^1]: [https://mb-cdi.stanford.edu/adaptations.htm](https://mb-cdi.stanford.edu/adaptations.htm)

```{r cdi-overlap, echo=F, include=F}
# WS production params with AGE
#load(here("data/xling-age-WS-prod-IRTparms.Rdata"))
#xldf <- xldf %>% mutate(uni_lemma = ifelse(uni_lemma=="NA", NA, uni_lemma))
#xldf_age <- xldf

# long data-frame of all WS production parameters
load(here("data/xling-WSprod-IRTparms.Rdata"))
nas <- xldf %>% filter(uni_lemma=="NA") # how did these end up with string "NA" values?
# distributed fairly well across languages; Slovak and Hungarian have >50 -- Persian has 277!
sort(table(nas$language)) 

xldf <- xldf %>% mutate(uni_lemma = ifelse(uni_lemma=="NA", NA, uni_lemma)) %>%
  filter(!is.na(d)) # removes 1 Slovak word with NA parameters (because all responses were 0)

# compare age and regular
#xlxl <- xldf %>% left_join(xldf_age %>% select(item_id, language, a1, d) %>% rename(a1_age = a1, d_age = d))
```

Given that it is well-established that, all over the world, early learned words reflect the people and things that children are likely to experience, that is, words for family members, animals, and common household objects [@tardif2008baby; @frank2021], it is reasonable to ask: Is there a single set of translation equivalents that would meet the criteria for inclusion on CDIs from multiple languages? 
To facilitate this effort, it is useful to leverage recent work using Item-Response Theory [IRT, @embretson2013] models. 
IRT models infer both the abilities of test takers and the difficulty of individual test items (i.e., words), along standardized dimensions. 
Recent work using IRT models have facilitated our understanding of the psychometric properties of CDI instruments. 
As such, they offer the potential to not only yield more accurate measures of children's language ability, but also to enable the construction of Computerized Adaptive Tests (CATs), which choose the next test item based on the responses to the previous items, and thus quickly hone in on the test-takers language ability. 
CAT-based CDIs presenting 50 or fewer items have been found to strongly correlate with scores on the full CDI:WS [@chai2020;@mayor2019;@Makransky2016]. 
A general method for creating CDI CATs that work well across a broader age range (12-36 months) has been proposed, and tested for American English and Mexican Spanish [@Kachergis2022]. 
However, the IRT model driving each CAT needs to be trained on a large and normative dataset, which may not be available in a given language. 
To date, the IRT models are also fitted separately for each language, and the fitted parameters (e.g., word difficulty) are likely to vary across languages.

The goal of the current study is to use IRT modeling in conjunction with data from Wordbank to examine whether there might be a core set of concepts that are frequently included on CDIs, and--importantly--whether many of them are of roughly equal difficulty across many languages. 
This work takes its inspiration from the fields of lexicostatistics and glottochronology, where researchers [notably, @Swadesh1971] have proposed a list of 'universal concepts'--concepts that exist in all catalogued languages--in order to quantify the genealogical relatedness and dates of divergence of languages. 
For example, the original Swadesh list contains 100 words, comprised of categories including common pronouns (“I”, “you”, “we”), animals (“man”, “fish”, “bird”, “dog”), objects (“tree”, “leaf”, “sun”, “mountain”), and verbs (“kill”, “die”, “see”, “sleep”). 
Extending this work to the development of a universal CDI, or “Swadesh CDI,” would include all of the concepts that researchers have chosen to include on the vast majority of CDI:WS adaptations, and which have relatively similar difficulty across many languages. 

More broadly, this work examines which types of words (and their corresponding concepts) are more or less similar across languages in terms of the ease with which they are learned, revealing commonalities as well as idiosyncrasies in children’s early experiences. 
We can ask: Which semantic categories, e.g., animals, household objects, food and drink, or another category, are most consistently learned across languages? Which are more variable? 
These types of cross-linguistic comparisons may give new insight into theoretical questions surrounding the similarity and differences between language experience and development in different cultural and linguistic contexts.

In particular, our contributions are 1) to revise and extend a set of translation-equivalent concepts ("universal lemmas", i.e. uni-lemmas) in Wordbank, 2) to fit IRT models to 28 CDI:WS datasets, 3) to identify 163 candidate "Swadesh CDI" items from a cross-linguistic comparison of concept difficulty and inclusion, and 4) to test how well this Swadesh CDI generalizes to a set of 8 additional low-data languages. 
We end by making a concrete proposal for how this Swadesh CDI list could be used in creating future CDI adaptations, and by discussing the strengths and weaknesses of our approach.


# Methods

## Item Response Theory

A variety of IRT models targeting different types of testing scenarios have been proposed [see @Baker2001 for an overview], but for the dichotomous responses that parents make for each item (word) regarding whether their child can produce a given word, we will use the popular 2-parameter logistic (2PL) model that is best justified for CDI data [see @Kachergis2022].

The 2PL model jointly estimates for each child $j$ a latent ability $\theta_j$ (here, language skill), and for each item $i$ two parameters: the item's difficulty $b_i$ and discrimination $a_i$, described below.
In the 2PL model, the probability of child $j$ producing a given item $i$ is 
 
 $$P_{i}(x_i = 1 | b_{i},a_{i},\theta_j ) = \frac{1}{1 + e^{-D a_{i}(\theta_j - b_i )}}$$

where $D$ is a scaling parameter ($D=1.702$) which makes the logistic more closely match the ogive function used in a standard factor analysis [@R-mirt; @reckase2009].
Children with high latent ability ($\theta$) will be more likely to produce any given item than children with lower latent ability, and more difficult items will be produced by fewer children (at any given $\theta$) than easier items.
The discrimination ($a_i$) adjusts the slope of the logistic (in the classic 1-parameter logistic (1PL or Rasch) model, the slope is always 1). 
Items with higher discrimination (i.e. slopes) better distinguish children above vs. below that item's difficulty level, and hence are generally more useful.
While other standard IRT models exist (e.g., the 3-parameter logistic model adds a 'guessing' parameter for each test item), a recent study found the 2PL model most appropriate for multiple Wordbank datasets [@Kachergis2022].

## Datasets

```{r, load-wg-data, eval=F, echo=F}
# WG production fits
load(here("data/multiling_2pl_WG_prod_fits.Rdata"))
wg_coefs = coefs
languages = names(coefs)
wg_tab <- get_item_n_subject_counts(models)
#sort(languages)

#load(here("data/xling_WG_abilities.Rdata"))
load(here("data/xling-WGprod-IRTparms.Rdata"))
# 24 languages, 9611 parameters
wg_parms <- xldf

```

```{r, load-ws-data, echo=F, results="asis"}
# WS production fits
load(here("data/multiling_2pl_WS_prod_fits.Rdata"))
#ws_coefs = coefs

ws_tab <- get_item_n_subject_counts(models)

#load(here("data/xling-WSprod-IRTparms.Rdata"))
#ws_parms <- xldf # 23179

xldf <- xldf %>% mutate(d = -d) # easiness -> difficulty

# WG langs that don't have WS: British Sign Language, Spanish (Chilean), American Sign Language, English (British)

it_tab <- ws_tab %>% #rename(`WS items`=items, `WS N`=N) %>%
  #left_join(wg_tab %>% rename(`WG items`=items, `WG N`=N)) %>%
  arrange(desc(`N`))

languages = subset(ws_tab, N>=200)$Language
low_data_langs <- it_tab %>% filter(`N`<200)

tab1 <- xtable::xtable(it_tab, digits=c(0), 
                       caption = "Number of CDI:WS items and subjects (N) per dataset.")
print(tab1, type="latex", comment = F, table.placement = "H", hline.after=26,
      include.rownames=FALSE)
```


We report IRT analyses for twenty-six languages from Wordbank [@frank2017], comprising production data from CDI:WS vocabulary checklists that have at least 200 administrations.[^2]
Data from the first twenty-six rows of Table 1 (Norwegian through Dutch) will be used to select a pool of words with approximately equal cross-linguistic difficulty.
CDI:WS production data from an additional eight languages (bottom of Table 1: Greek (Cypriot) through Persian) had too few participants to be analyzed with IRT.
Datasets for these languages will be used to test how well the selected pool of words generalize can be expected to new languages.

### Uni-lemmas

In order to do so, we need a method to map between words across languages that correspond to broadly similar concepts. 
As such, we mapped each item on the CDI:WS for each language onto a set of “universal lemmas” or “unilemmas”, which are approximate cross-linguistic conceptual mappings of words. 
For example, “dog” (English) and “perro” (Spanish) both correspond to the same unilemma, dog. 
These mappings were recently updated for Wordbank 2.0 to improve their quality and systematicity, and to increase coverage across items and languages. 
This new set of unilemmas was constructed based on glosses provided by the original contributors of the Wordbank datasets, which were then verified by native or advanced proficient speakers of the language, and cleaned to increase their consistency across languages. 
All unilemmas are accessible from Wordbank; details about the recent update can be found at [https://github.com/langcog/update_unilemmas](https://github.com/langcog/update_unilemmas).

### Participants


```{r participant-data, eval=F, echo=F}
demo <- tibble()
for(lang in languages) {
  load(here(paste("data/WS/",lang,"_WS_data.Rdata", sep='')))
  demo <- bind_rows(demo, d_demo %>% 
                      select(data_id, age, production, language))
}

save(demo, file=here("data/WS_combined_demo.Rdata"))
```

```{r combined-demo-data, echo=F}
load(here("data/WS_combined_demo.Rdata"))
# cor.test(demo$age, demo$production, na.rm=T) # r=.70

# 45402 in the 26 high-data languages
demo_age <- demo %>% group_by(language) %>%
  summarise(n = n(), age = mean(age))

#demo %>% 
#  ggplot(aes(x=age, fill=language, color=language)) + 
#    geom_histogram(position="identity", alpha=0.2)
```


The CDI:WS production dataset consists of the combined Wordbank production data for `r sum(ws_tab$N)` children aged 16-30 months on `r sum(ws_tab$items)` items across `r nrow(ws_tab)` forms.
Note that the distributions of demographic variables (age, sex, maternal education, etc.) of these datasets are not matched, so comparing overall language ability estimates across languages would be impossible. 
(See @frank2021 for a discussion of effects of demographic variables on vocabulary development.) 
Thus, we will focus only on the estimated item parameters, and in particular the variability of item difficulty ($b_i$).

[^2]: [http://wordbank.stanford.edu/contributors](http://wordbank.stanford.edu/contributors)



### Instruments

<!-- The 680-item vocabulary checklist of the English CDI:WS form is organized into 22 semantic categories (e.g., furniture, games and routines, people). -->
<!-- When a CDI:WG form was administered, caregivers were asked to indicate for each vocabulary item whether their child 1) understands that word ("comprehends") or 2) both understands and says ("produces") that word. 
Leaving the item blank indicates that the child neither comprehends nor produces that word. -->
When a CDI:WS forms was administered, caregivers were asked to indicate for each vocabulary item on the instrument whether or not their child can recognizably produce (say) the given word.

"Produces" responses were coded as 1 and all other responses were coded as 0.
Our datasets consist of a dichotomous-valued response matrix for each language, of size $N$ subjects $\times$ $W$ words.

# Results

```{r}
xldf <- xldf %>% filter(!is.na(uni_lemma)) # only want the words with defined uni-lemmas
xldf_lowd <- xldf %>% filter(is.element(language, low_data_langs$Language)) # 5464
xldf <- xldf %>% filter(!is.element(language, low_data_langs$Language)) # 17715

# align some categories: Russian has descriptive adjectives and adverbs
xldf[which(xldf$category=="descriptive_words (adjectives)"),]$category = "descriptive_words"
xldf[which(xldf$category=="outside_places"),]$category = "outside"
```


All models, data, code for reproducing this paper are available on OSF[^3].
Across the 26 IRT models for different CDI:WS forms, parameters for a total of `r nrow(xldf)` items were obtained.

```{r fig-mean-item-diff, include=F, fig.cap=c("Mean item difficulty for each CDI:WS form. Bars represent bootstrapped 95\\% confidence intervals. Color shows mean age (months) of the sample, which is correlated with mean word difficulty (r=.57)."), fig.height=4.3, fig.width=3.4}
# re: Philip's question of are item difficulties comparable across languages:
lang_d_m <- xldf %>% filter(!is.na(d)) %>% # one Slovak word has NA parms..
  group_by(language) %>% 
  tidyboot::tidyboot_mean(d, na.rm=T) %>%
  arrange(desc(mean))

lang_d_m <- lang_d_m %>% left_join(demo_age %>% select(language, age))
# problem: average item difficulty per language is strongly related to mean sample age
# cor.test(lang_d_m$mean, lang_d_m$age) # r=.57  t(24) = 3.44, p = 0.002

lang_d_m %>% ggplot(aes(x=reorder(language, -mean), y=mean)) + coord_flip() + 
  geom_point(aes(color=age)) + theme_classic() +
  geom_linerange(aes(ymin=ci_lower, ymax=ci_upper), alpha=.7) +
  theme(legend.position="bottom") +
  xlab("Language") + ylab("Mean item difficulty")
```


[^3]: OSF repository: [https://osf.io/8swhb/](https://osf.io/8swhb/?view_only=6f6ab9818f2a4bb288e05ca9e12f540c).


<!-- ## Difficulty by Semantic Category -->

Figure 1 shows the average cross-linguistic difficulty of CDI items by semantic category.
Sounds (e.g., animal noises), body parts, and common nouns tend to be early-learned, and thus have lower difficulty values, while abstract words such as time words and morphologically complex helping verbs are later-learned (i.e., more difficult).

```{r difficulty-by-category, fig.env = "figure", fig.pos = "h", fig.height=3.5, fig.align = "center", fig.cap = "Mean cross-linguistic difficulty of CDI words by semantic category. Bars represent bootstrapped 95\\% confidence intervals."}

# remove categories that only exist in 1 or 2 languages:
prod_cat <- xldf %>% 
  filter(!is.na(d), !is.na(category),
         !is.element(category, c("final_particles", "directions", "numbers", "articles", "other",
                                 "descriptive_words (adverbs)", "states", "unknown",
                                 "verb_endings", "verb_modifiers",
                                 "classifiers", "locations_quantities_adverbs"))) %>%
  group_by(language, category) %>%
  summarise(d = mean(d), a1 = mean(a1))

# mean variability of each category, across languages
prod_cat %>% group_by(category) %>%
  tidyboot::tidyboot_mean(d, na.rm=T) %>%
  ggplot(aes(x=reorder(category, mean), y=mean)) + geom_point(alpha=.7) +
  geom_linerange(aes(ymin=ci_lower, ymax=ci_upper), alpha=.7) +
  coord_flip() + theme_classic() + ylab("Mean item difficulty") + xlab("Category")
```



### Identifying Swadesh CDI Candidates

```{r, include=F}
# 1840
uni_ag <- xldf %>% filter(uni_lemma!="NA") %>%
  group_by(uni_lemma, category) %>%
  summarise(d_m=mean(d), a1_m=mean(a1), d_sd = sd(d), a1_sd = sd(a1), n=n())

# The more languages a word is in, the easier it tends to be:
cor.test(uni_ag$d_m, uni_ag$n)
# r=-.27, t(2441) = -13.89, p<.001
plot(uni_ag$d_m, uni_ag$n)

# variability in difficulty also correlated number of forms uni-lemma is on
cor.test(uni_ag$d_sd, uni_ag$n) 
# r=.17, t(1451) = 6.38,  p<.001
# The more forms a uni-lemma appears on, 1) the easier it tends to be (r=-.27), and 2) the higher the variability in its difficulty (r=.17.

# 990 singletons
uni1 <- uni_ag %>% filter(n==1) 
mean(uni1$d_m) # 1.21

uni_ag <- uni_ag %>% filter(n>1) # 1453

# no cor bw difficulty and variability -- good
cor.test(uni_ag$d_m, uni_ag$d_sd)

#xldf %>% group_by(language, uni_lemma)
```


```{r, echo=F, message=F}
pars <- xldf %>% filter(!is.na(uni_lemma), !is.na(d)) 

ul <- sort(table(pars$uni_lemma)) 

long_list = ul[which(ul>1)] # 1311
short_list = ul[which(ul>9)] # 655 uni-lemmas defined in 10+ languages
```

```{r}
# problem: some languages have multiple items matching a single uni_lemma
# (e.g., Croatian has stric=uncle and ujak=uncle; Turkish has 3 "market"s )
prod_pars <-
  pars %>% arrange(desc(language), desc(uni_lemma), desc(a1)) %>% # get most discriminating uni_lemma per lang
  #filter(is.element(uni_lemma, names(short_list))) %>%
  select(uni_lemma, category, lexical_category, language, d) %>%
  group_by(uni_lemma, language) %>%
  slice(1) %>% 
  pivot_wider(names_from = language, values_from = d)
```


CDI:WS forms have a median of 693 uni-lemmas defined (range: 553 (Czech) to 804 (Cantonese)), and there are a total of `r length(unique(xldf$uni_lemma))` uni-lemmas defined across the 26 languages.
Only `r nrow(na.omit(prod_pars))` uni-lemmas appear on all `r length(languages)` CDI:WS forms: too few to comprise a short form.
To expand the pool of items, we consider thresholds on both 1) the variability of an item's difficulty across languages (lower is better), and 2) the number of CDI:WS forms on which it appears (more is better).
Below we examine the standard deviation of uni-lemmas' cross-linguistic difficulty as a function of how many languages that uni-lemma is missing from.
For now we consider items with less than median variability in difficulty that are included on 10 or more of the 26 CDI:WS forms.

```{r old-swadesh-list-method, echo=F, include=F}
prod_pars$sd = apply(prod_pars[,4:ncol(prod_pars)], 1, sd, na.rm=T)
prod_pars$numNAs = apply(prod_pars[,4:ncol(prod_pars)], 1, function(x) { sum(is.na(x)) }) 

# median SD of the item difficulty
med_d = median(prod_pars$sd, na.rm=T) # 1.14
# SD of the SD
sd_d = sd(prod_pars$sd, na.rm=T) # .54

prod_pars %>% filter(sd<4) %>%
  ggplot(aes(x=jitter(numNAs), y=sd, color=lexical_category)) + 
  geom_point(alpha=.7) + theme_classic() + 
  xlab("# of languages without uni-lemma") +
  ylab("SD of unilemma's cross-linguistic difficulty") +
  geom_hline(aes(yintercept=med_d - .5*sd_d), linetype="dashed") + 
  geom_vline(aes(xintercept=6.5), linetype="dashed")

# only 38 now!
good_prod <- prod_pars %>% 
  filter(numNAs < 16) %>%
  filter(sd < (med_d - .5*sd_d)) %>% 
  arrange(numNAs) 

#sort(table(good_prod$category)) / nrow(good_prod)
#sort(table(good_prod$lexical_category)) / nrow(good_prod)
```


```{r grid-search, eval=F}
# how many Swadesh words when looking at all uni-lemmas included in at least k languages?
# (same mean(sd) - sd(sd) threshold applied to all, but could make that a grid, e.g. sd_thresh = seq(0, 2, .25)
swad_lists <- list()
swad_agg <- tibble()

# only want 1 
uni_per_form <- xldf %>% arrange(desc(language), desc(uni_lemma), desc(a1)) %>% # get most discriminating uni_lemma per lang
  select(uni_lemma, category, lexical_category, language, d) %>%
  group_by(uni_lemma, language) %>%
  slice(1) %>%
  group_by(uni_lemma) %>%
  summarise(d_m = mean(d), d_sd = sd(d), n = n()) %>%
  filter(!is.na(d_m)) # one uni-lemma: "look for"

# has to be at least 2 forms, as SD only defined for 2+
for(k in 2:26) {
  uni_k <- uni_per_form %>%
    filter(n >= k)
  N = nrow(uni_k)
  mean_diff = mean(uni_k$d_m) # average difficulty of uni-lemmas on at least k forms
  mean_sd = mean(uni_k$d_sd) # average SD of difficulty of uni-lemmas on at least k forms
  sd_sd = sd(uni_k$d_sd) # variability of SD(diff)
  sd_thresh = mean_sd - .5*sd_sd
  swad_lists[[k]] = uni_k %>% filter(d_sd < sd_thresh)
  
  xx_g <- run_comparisons(xldf, languages |> sort(), swad_lists[[k]]$uni_lemma, 
                        rand_method = "unilemmas_weighted", rand_comparisons = 1000,
                        metrics = c("sumscore_cor", "test_info")) # theta_cor breaks on French (French): incompatible dimensions
  
  xx_agg <- xx_g |> 
    group_by(language, sublist, metric) |> 
    summarise(mean = mean(value),
              N_avail = mean(N)) |> 
    spread(metric, mean) |>
    mutate(k = k, 
           N = N,
           mean_diff = mean_diff,
           mean_sd = mean_sd,
           sd_sd = sd_sd,
           sd_thresh = sd_thresh,
           swad_mean_diff = mean(swad_lists[[k]]$d_m),
           swad_diff_sd = sd(swad_lists[[k]]$d_m),
           swad_n = nrow(swad_lists[[k]]))
  
  swad_agg <- bind_rows(swad_agg, xx_agg)
}



# which items crop up over and over? (have to decide which items to discuss in the paper!)
all_swad <- c()
for(k in 2:26) {
  all_swad = c(all_swad, swad_lists[[k]]$uni_lemma)
}
swad_freq = sort(table(all_swad), decr=T)

save(swad_agg, swad_lists, swad_freq, file=here("data/Swadesh_grid_search_final.Rdata"))

#library(wordcloud)
#library(RColorBrewer)
#wordcloud(words = names(swad_freq), freq = swad_freq, min.freq = 1,           
#          max.words=100, random.order=FALSE, rot.per=0.35, scale=c(3.5,0.25),
#          colors=brewer.pal(8, "Dark2"))

library(wordcloud2)
wordcloud2(data=data.frame(word = names(swad_freq[1:150]), 
                           freq = as.vector(swad_freq[1:150]-13)), size=.5, color='random-dark')
```

```{r load-swadesh-lists, echo=F}
load(here("data/Swadesh_grid_search_final.Rdata"))

swad_freq[1:50] # appearing on 22-24 Swadesh lists
swad_freq[51:100] # 18-22 lists
swad_freq[101:153] # 14-18

# looking at appendix table, k=16 seems the best (highest test info, v close to same full correlation as rand)
good_prod = swad_lists[[16]]
```

```{r fig-mean-swadesh-item-diff, fig.cap=c("Mean item difficulty of Swadesh CDI items per language. Bars represent bootstrapped 95\\% confidence intervals."), fig.height=3.5, fig.width=3.4}

swad_d_m <- xldf %>% filter(is.element(uni_lemma, good_prod$uni_lemma)) %>% # 
  group_by(language) %>% 
  tidyboot::tidyboot_mean(d, na.rm=T) %>%
  arrange(desc(mean))


swad_d_m %>% ggplot(aes(x=reorder(language, -mean), y=mean)) + coord_flip() + 
  geom_point() + theme_classic() +
  geom_linerange(aes(ymin=ci_lower, ymax=ci_upper), alpha=.7) +
  theme(legend.position="bottom") +
  xlab("Language") + ylab("Mean item dificulty")
```

Of these, a total of `r length(long_list)` uni-lemmas are included on more than one form, and only `r length(short_list)` uni-lemmas are included on 10 or more of the CDIs.
This restricted list is used as a starting point for selecting the Swadesh CDI candidates. 

The next criterion for Swadesh candidates is to find uni-lemmas of consistent difficulty across languages:
if an item has relatively similar difficulty across 10 or more languages, we expect that it is more likely to have similar difficulty in a new language, compared to an item that has more variability.
To evaluate how variable uni-lemmas are in their cross-linguistic difficulty, we calculate the standard deviation (SD) of each uni-lemma's difficulty.
The median SD is `r round(med_d, 2)` (SD=`r round(sd_d, 2)`), so we consider the items with SD less than half the median SD (i.e., SD < `r round(med_d - .5*sd_d, 2)`).
These `r nrow(good_prod)` candidate "Swadesh CDI" items are shown in Figure 3, grouped by semantic category.
The Swadesh CDI candidates represent 17 of the 22 semantic categories on the original American English CDI:WS form: the five categories not represented are toys, pronouns, question words, quantifiers, and helping verbs.
51% of the Swadesh CDI words are nouns, 21% are predicates, 6% are function words, and 22% are other.
This breakdown is comparable to the lexical category percentages on the 680-item English CDI:WS (46% nouns, 24% predicates, 15% function words, and 5% other), minimally suggesting that this method of selecting candidate items is not biased by lexical category.
The Swadesh candidates were also present on more forms than typical in the selection set: on average, each item appeared on `r round(26-mean(good_prod$numNAs), 0)` forms.

```{r, swadesh-vs-non-swadesh-diff, echo=F}
xldf <- xldf %>% 
  mutate(SwadeshCDI=ifelse(is.element(uni_lemma, good_prod$uni_lemma), 1, 0))

tmp <- xldf %>% 
  group_by(SwadeshCDI) %>% 
  filter(!is.na(d)) %>%
  summarise(sd_d = sd(d), d = mean(d), 
            sd_a1 = sd(a1), a1 = mean(a1))
# mean difficulty of Swadesh words: -0.16; mean easiness of other words: 0.58
# (mean of all: mean(xldf$d, na.rm=T) .42

#t.test(subset(xldf, SwadeshCDI==1)$a1, subset(xldf, SwadeshCDI==0)$a1) 
# Swadesh words have higher discrimination!
# t(5983) = 2.91, p=.004

#t.test(subset(xldf, SwadeshCDI==1)$d, subset(xldf, SwadeshCDI==0)$d)
# t = -22.587, df = 6832.4, p-value < 2.2e-16

gen_test_langs <- low_data_langs$Language
```

Comparing the IRT parameters of the Swadesh CDI uni-lemmas to the rest of the items showed that the discrimination parameter (i.e., slope) of the Swadesh items did not significantly differ from the others.
However, the candidate Swadesh items were significantly easier than the other uni-lemmas (mean Swadesh $d=$`r round(tmp[2,"d"], 2)`, other items' mean $d=$`r round(tmp[1,"d"], 2)`, $t(3123)=15.63$, $p<.001$).
It is worth noting that three of the five semantic categories not appearing on the Swadesh list are among the most difficult: helping verbs, question words, and pronouns. 
It may be prudent to consider expanding the Swadesh list to include some of these more difficult (and variable) items.
Returning to the list of uni-lemmas that appear on at least four forms, we identified 10 uni-lemmas that cover the unrepresented difficult categories, and still fall below the variability threshold.
The mean difficulty of these items is 3.13, far higher than that of other uni-lemmas.
We will test the Swadesh candidates with and without this _difficult_ list.
<!-- connective: "then" 
     pronouns: "2SG.REFL" "3PL.POSS" "these"
    helping verbs: "can" "would" "am" "have to"
    quantifier: "any" 
    question word: "how many" -->

```{r, echo=F, results='asis', include=F}
good_prod2 <- good_prod %>% left_join(xldf %>% distinct(uni_lemma, category, lexical_category))
#sort(table(good_prod2$category), decr=T)
#sort(table(good_prod2$lexical_category), decr=T)
# nouns, other, predicates, function words
# c(119,55,55,30) / 259

# table(subset(xldf, language=="English (American)")$lexical_category) / 680
# function_words     nouns      other    predicates 
#    0.15          0.46      0.1470588      0.24

#kableExtra::kable(good_prod, digits=2)
#good_prod %>% DT::datatable() %>%
#  DT::formatRound(columns=4:24, digits=3)
tab2 <- good_prod %>% group_by(category) %>%
  summarise(n=n()) %>%
  arrange(desc(n))

tab2 <- xtable::xtable(tab2, digits=c(0), 
                       caption = "Number of proposed Swadesh CDI items by semantic category.")
print(tab2, type="latex", comment = F, table.placement = "H", include.rownames=FALSE)
```

```{r difficult-words, echo=F}
# want sd(d)<
#median(uni_ag$d_sd, na.rm=T) # 1.14
# find additional low-variability words from missing categories 
missing_cats = c("pronouns", "question_words", "quantifiers", "helping_verbs", "connecting_words")
low_var <- uni_ag %>% 
  filter(d_sd<.96, n>3, is.element(category, missing_cats), uni_lemma!="and") %>% 
  arrange(desc(n))
#mean(low_var$d_m) # 3.13
```


## Comparing Swadesh CDI to Full CDI:WS

How well do raw scores from the Swadesh CDI items correlate with full CDI:WS scores?
On average, for the 26 languages the IRT analysis was based on, the Swadesh CDI's sumscores were strongly related to the full CDI:WS scores (mean $r=0.990$; $min=0.976$, $max=0.996$; full table on [OSF](https://osf.io/8swhb/?view_only=6f6ab9818f2a4bb288e05ca9e12f540c)).
However, these correlations were slightly but significantly lower than scores based on randomly-selected items (mean $r=0.993$, $min=0.990$, $max=0.996$, paired $t(25) = 5.67$, $p<.001$).
This is likely due to some ceiling effects for older children on the Swadesh CDI, since the randomly-selected items are likely somewhat easier on average than the Swadesh items (see above results).

Adding the 10 difficult uni-lemmas improved the relationship (mean $r=.991$).


## Testing Generality of the Swadesh CDI 

For the eight low-data languages, a similar comparison revealed that the Swadesh CDI's raw scores were again strongly related to the full CDI:WS scores (mean $r=0.978$; $min=0.959$, $max=0.992$).
As with the training set, however, the Swadesh list correlations were slightly but significantly lower than scores based on randomly-selected items (mean $r=0.985$; $min=0.972$, $max=0.992$; $t(7) = 3.22$, $p=.01$).

Adding the 10 difficult uni-lemmas improved the relationship (mean $r=.984$).

```{r generalization-test, echo=F, eval=F}
# use the good_prod words as a short CDI, and compare the correlation 
# of these words to a random selection (might be worse, due to being biased towards easy words?)

# just the Swadesh
xx <- run_swadesh_comparisons(xldf, languages, good_prod$uni_lemma)
#summary(xx$`Swadesh r`)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.9852  0.9891  0.9925  0.9915  0.9933  0.9964
#summary(xx$`Rand r`)
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.9952  0.9963  0.9969  0.9968  0.9973  0.9978 


gen_xx <- run_swadesh_comparisons(xldf_lowd, gen_test_langs, good_prod$uni_lemma)
#summary(gen_xx$`Rand r`)
#      Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 #0.9863  0.9937  0.9942  0.9935  0.9947  0.9965 

#summary(gen_xx$`Swadesh r`) 
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0.9788  0.9808  0.9861  0.9856  0.9881  0.9962 


save(xx, gen_xx,   
     file=here("data/Swadesh_comparisons.Rdata"))
```

```{r gen-results, results='asis'}
load(here("data/Swadesh_comparisons.Rdata"))

gen_tab <- xtable::xtable(gen_xx, digits=c(3), 
                       caption = "Generalization test results.")
print(gen_tab, type="latex", comment = F, table.placement = "H",
      include.rownames=FALSE)
```




```{r, include=F}

# are we any worse than random? slightly, but significantly...
t.test(xx$`Swadesh r` - xx$`Rand r`)
# t(25) = -8.06, p<.001; diff=-.004

t.test(xx_ext$`Swadesh r` - xx_ext$`Rand r`)
# t(25) = -6.38, p<.001, diff=-.003

t.test(gen_xx$`Swadesh r` - gen_xx$`Rand r`) 
# t(7) = -4.83, p=0.002;  diff=-.01

t.test(gen_xx_ext$`Swadesh r` - gen_xx_ext$`Rand r`) 
# t(6) = -3.23 diff=-.005

short_wsA <- read_csv(here("data/eng-ws-shortA.csv"))
short_wsB <- read_csv(here("data/eng-ws-shortB.csv"))

sort(intersect(short_wsA$word, good_prod$uni_lemma)) # 14
# "airplane" "broom"    "cat"      "dog"      "meow"     "milk"     "necklace"      
# "peas"     "school"   "shopping" "sky" "star"     "tonight"  "under"   
sort(intersect(short_wsB$word, good_prod$uni_lemma)) # 8
# "black"    "catch"    "dirty"    "hose"     
# "mailman"  "nose"     "tomorrow" "tongue" 

sort(short_wsA$word)
sort(good_prod$uni_lemma)
# maybe remove? penis, vagina, on the basis of DIF
```


```{r, out.width="\\linewidth", include=TRUE, fig.align="center", fig.cap=c("The 163 proposed Swadesh CDI words by semantic category."), echo=FALSE}
knitr::include_graphics("figs/SwadeshCDI_list.pdf")
```



```{r swadesh-comparison, echo=F}
# https://en.wikipedia.org/wiki/Swadesh_list
swadesh100 = c("I","you","we","this","that","who","what","not","all","many","one","two","big","long","small",
               "woman","man","person","fish","bird","dog","louse","tree","seed","leaf","root","bark","skin","flesh","blood",
               "bone","grease","egg","horn","tail","feather","hair","head","ear","eye","nose","mouth","tooth","tongue","claw",
               "foot","knee","hand","belly","neck","breasts","heart","liver","drink (action)","eat","bite","see","hear","know","sleep",
               "die","kill","swim","fly","walk","come","lie","sit","stand","give","say","sun","moon","star","water",
               "rain","stone","sand","earth","cloud","smoke","fire","ash","burn","path","mountain","red","green","yellow","white",
               "black","night","hot","cold","full","new","good","round","dry","name")

# https://en.wikipedia.org/wiki/Automated_Similarity_Judgment_Program#Word_list
# ASJP subset of 40 Swadesh items that perform as well as full list
# categories: Body parts, Animals and plants, People, Nature, Verbs and adjectives, Numerals and pronouns
asjp = c("eye","ear","nose","tongue","tooth","hand","knee","blood","bone","breast","liver","skin",
         "louse", # "bug" ?
         "dog","fish (animal)","horn","tree","leaf",
         "person","name", # People; match "name" to "child's own name" / "babysitter's name" / "pet's name" ?
         "sun","star","water (not beverage)","fire","stone","path","mountain","night", # Nature
         "drink (action)","die","see","hear","come", # Verbs and adjectives
         "new","full","one","two","I","you","we")

#intersect(asjp, good_prod$uni_lemma) 
# "nose"   "tongue" "tooth"  "dog"    "star" 
```



# Discussion

This study compared psychometric models fitted to 26 CDI datasets in order to find concepts that have low variability in their cross-linguistic difficulty, and that are frequently included on parent-reported measures of children's early vocabulary. 
We identified 163 concepts that appeared on at least 10 of the CDIs, and which have more consistent cross-linguistic difficulty than the majority of the concepts appearing on multiple CDIs.
We showed that using this set of "Swadesh CDI" items as a short assessment would generate scores that are strongly related to full CDI:WS scores, both for the original 26 datasets, and in a generalization test to eight low-data languages.
The Swadesh CDI contains items with relatively good difficulty estimates cross-linguistically, and in the absence of access to researchers who are familiar with relevant local cultures and concepts, they may serve as a rapid, simple means of approximating children's ability levels, even in the absence of a large norming dataset.

However, the Swadesh CDI items were also significantly easier than other items, meaning that older children may perform at ceiling if given only the Swadesh CDI items.
(This may be unsurprising from the perspective that Swadesh words are meant to be universal, and are therefore more frequent and basic---both within and across individual children's experiences.)
Thus, our suggested use case for the Swadesh CDI list is as a starting point for researchers seeking to develop a CDI in a new language, rather than as a complete short-form CDI.
In particular, researchers should seek to add contextually appropriate items from the categories that were notably absent from the Swadesh list: quantifiers, .. 
<!--Note that something interesting is how researchers cannot a priori know which items are difficult, so we can't recommend that they "add more difficult items"... I guess they can look at more difficult _uni-lemmas_ and add some of those if they are contextually appropriate?? Not sure exactly what the best recommendation is.-->

Another potential limitation of this work is that most existing CDIs (and most datsets available in Wordbank) target languages in the Indo-European language family.
It is not clear to what extent this bias in the existing data might interfere with generalizing to non-Indo-European languages.
Nonetheless, our original 26 datasets include 7 non-Indo-European languages (3 Sino-Tibetan, 1 Afro-Asiatic, 1 Uralic, 1 Koreanic, 1 Turkic), and the generalization datasets include 1 Uralic language and 2 Niger--Congo languages (a langauge family that is not even represented in the original datasets); the broad consistency across language families thus suggests that the effectiveness of the Swadesh list may be sufficiently robust.
<!--Such a robustness also corroborates with results from @floccia2018, who demonstrated that a bilingual vocabulary model based on a set of 30 uni-lemmas had good predictive validity on non-target languages, even those from other language families.-->

Developing a list of appropriate vocabulary words is not the only challenge researchers face when seeking to develop and use parent-report measures in a new language and culture. 
The pragmatics of language between children and adults can differ greatly across cultures, and has been found to interfere with administration of parent-report measures of early vocabulary, for example in Kiswahili [@alcock2017production] and Wolof [@weber2018].
As such, local cultural knowledge remains essential in appropriately developing and administering novel CDI adaptations.

Despite the myriad challenges that remain in creating new measures of early language development, we believe that the proposed Swadesh CDI list will give researchers a solid foundation to start from, lowering the barrier to the adaptation of CDI forms in new languages.
Certainly, increasing the diversity of languages studied is a critical step towards developing a truly general understanding of how young children learn language.

# Acknowledgements

We would like to thank all of the contributors to Wordbank, from the researchers who created and adapted the CDIs to those who collected the data (as well as the participants), to those who have created and maintained Wordbank over the years. 

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
