---
title: "Swadesh: New analyses"
format: html
---

```{r setup, include=FALSE}
library(tidyverse)
library(mirt)
library(glue)
library(here)
library(plotly)
options(dplyr.summarise.inform = FALSE)
theme_set(theme_classic())
set.seed(42)
```

### Collapse cross-linguistic item parameters
```{r}
languages <- list.files("data/all_forms") |> str_sub(end = -10)
xldf <- list()
for (language in languages) {
  lang_data <- readRDS(glue("data/all_forms/{language}_data.rds"))
  fitted <- readRDS(glue("data/prod_models/{language}_2PL_allforms_prod_fits.rds"))
  df <- fitted$coefs |> 
    rename("uid" = "definition") |> 
    left_join(lang_data$items |> 
                select(uid, category, definition, gloss, uni_lemma),
              by = "uid") |> 
    mutate(language = language)
  xldf <- c(xldf, list(df))
}
xldf <- bind_rows(xldf)
saveRDS(xldf, "data/xldf_prod_allforms.rds")
```


### Calculate all theta values and plot by age
French (French) behaves weirdly with an unusual peak near 19mo.
```{r}
calc_theta <- function(languages) {
  xx <- tibble()
  for (language in languages) {
    # if(lang %in% c("Spanish (Mexican)", "French (French)")) next
    message(glue("Processing {language}\r"))
    lang_data <- readRDS(glue("data/all_forms/{language}_data.rds"))
    d_prod <- lang_data$all_prod
    bad_words_prod <- c(which(colSums(d_prod, na.rm=T) == 0), 
                        which(colSums(d_prod, na.rm=T) == colSums(!is.na(d_prod))))
    if(length(bad_words_prod) != 0) d_prod = d_prod[,-bad_words_prod]
    bad_ppts_prod <- c(which(rowSums(d_prod, na.rm=T) == 0))
    if(length(bad_ppts_prod) != 0) d_prod = d_prod[-bad_ppts_prod,]
    
    ids <- d_prod |> rownames() |> as_tibble() |> 
      rename(data_id = value) |> 
      mutate(data_id = as.double(data_id)) |> 
      left_join(lang_data$all_demo |> select(data_id, age), by = "data_id")
    
    fitted <- readRDS(glue("data/prod_models/{language}_2PL_allforms_prod_fits.rds"))
    thetas <- fscores(fitted$model, method = "MAP")
    thetas <- cbind(ids, thetas) |> 
      mutate(language = language)
    
    saveRDS(thetas, glue("data/allforms_thetas/{language}.rds"))
    xx <- xx |> bind_rows(thetas)
  }
  return(xx)
}
```

```{r eval=F}
all_thetas <- calc_theta(languages)
saveRDS(all_thetas, "data/allforms_thetas/all_thetas.rds")
```

```{r}
theta_plot <- ggplot(all_thetas, aes(x=age, y=G, col=language)) +
  theme_classic()

theta_plot +
  geom_point(alpha = .01, position="jitter") + 
  geom_line(stat = "smooth",
            method = "loess", 
            formula = y ~ x,
            se = F,
            alpha = .5)
```

#### Suspect non-linearity, so Spearman's correlations between G and age
```{r}
theta_cor <- all_thetas |> 
  group_by(language) |> 
  summarise(cor = cor(age, G, use = "complete.obs", method = "spearman"))
theta_corr <- all_thetas |> 
  group_by(language) |> 
  summarise(cor = cor(age, G, use = "complete.obs", method = "pearson"))
```


### Clean xldf
```{r}
xldf_clean <- xldf |> 
  filter(!is.na(uni_lemma), !is.na(d)) |> 
  mutate(category = case_when(
    category == "descriptive_words (adjectives)" ~ "descriptive_words",
    category == "outside_places" ~ "outside",
    .default = category))

prod_pars <- xldf_clean |> 
  arrange(language, uni_lemma, desc(a1)) |> # get most discriminating uni_lemma per lang
  select(uni_lemma, language, uid, category, language, d) |>
  group_by(uni_lemma, language) |>
  slice(1)

gen_langs <- c("Finnish",
               "Kigiriama",
               "American Sign Language",
               "Greek (Cypriot)",
               "Spanish (Peruvian)",
               "British Sign Language",
               "Persian",
               "Kiswahili",
               "English (Irish)",
               "Irish",
               "Spanish (Chilean)")

train_langs <- setdiff(languages, gen_langs)
```


```{r}
prod_plot <- prod_pars |> 
  filter(language %in% train_langs) |> 
  group_by(uni_lemma) |> 
  summarise(num_langs = n())

ggplot(prod_plot,
       aes(x = num_langs)) +
  geom_histogram(bins = 27)
```


### Cross-validation
```{r}
cv_res <- list()
S_LEN = 100
rand_comparisons = 100

for (lang in train_langs) {
  message(glue("Calculating for {lang}..."))
  
  # Swadesh: S_LEN smallest SDs
  prod_sum <- prod_pars |> 
    filter(language %in% train_langs,
           language != lang) |> 
    group_by(uni_lemma) |> 
    summarise(num_langs = n(),
              mean_d = mean(d, na.rm=T),
              sd_d = sd(d, na.rm=T))
  prod_test <- prod_pars |> 
    filter(language == lang)
  prod_cors <- sapply(2:(length(train_langs)-1), \(k) {
    prod_subset <- prod_sum |> 
      filter(num_langs >= k) |> 
      arrange(sd_d) |> 
      slice(1:S_LEN)
    prod_res <- prod_subset |> 
      left_join(prod_test, by = "uni_lemma")
    c((!is.na(prod_res$d)) |> sum(),
      cor(prod_res$mean_d, prod_res$d, use = "complete.obs"))
  }) |> t() |> 
    `colnames<-`(c("num_overlap", "cor")) |> 
    as_tibble() |> 
    mutate(run = NA,
           k = 2:(length(train_langs)-1),
           language = lang,
           sublist = "Swadesh")
  
  # random
  rand_cors <- lapply(2:(length(train_langs)-1), \(k) {
    rand_subk <- prod_sum |> 
      filter(num_langs >= k)
    rand_cors <- sapply(1:rand_comparisons, \(comp) {
      rand_idx <- sample(1:nrow(rand_subk), min(S_LEN, nrow(rand_subk)))
      rand_res <- rand_subk |> 
        slice(rand_idx) |> 
        left_join(prod_test, by = "uni_lemma")
      c((!is.na(rand_res$d)) |> sum(),
        cor(rand_res$mean_d, rand_res$d, use = "complete.obs"))
      }) |> t() |> 
      `colnames<-`(c("num_overlap", "cor")) |> 
      as_tibble() |> 
      mutate(run = 1:rand_comparisons,
             k = k)
  }) |> 
    bind_rows() |> 
    mutate(language = lang,
           sublist = "Random")
  
  cv_res <- c(cv_res, list(bind_rows(prod_cors, rand_cors)))
}
cv_res <- bind_rows(cv_res)

cv_res_sum <- cv_res |> 
  group_by(k, language, sublist) |> 
  summarise(num_overlap = mean(num_overlap),
            cor = mean(cor))
```

```{r}
ggplot(cv_res |> filter(sublist == "Swadesh"),
       aes(x = k, y = cor)) +
  geom_jitter(aes(col = language),
              alpha = .4) +
  geom_boxplot(aes(group = k)) +
  labs(y = "Correlation") +
  theme(legend.position = "none")
```

```{r}
cv_sum <- cv_res |> 
  group_by(k) |> 
  summarise(mean_overlap = mean(num_overlap),
            sd_overlap = sd(num_overlap),
            mean_cor = mean(cor),
            sd_cor = sd(cor))
```

```{r}
ggplot(cv_res |> filter(sublist == "Swadesh"),
       aes(x = k, y = num_overlap)) +
  geom_jitter(aes(col = language),
              alpha = .4) +
  geom_boxplot(aes(group = k)) +
  labs(y = "Overlap") +
  theme(legend.position = "none")
```


```{r}
ggplot(cv_res_sum,
       aes(x = k, y = cor, col = sublist)) +
  geom_jitter(aes(col = sublist),
              alpha = .1) +
  geom_boxplot(aes(group = interaction(k, sublist))) +
  labs(y = "Correlation") +
  theme(legend.position = "bottom")
```

```{r}
ggplot(cv_res_sum,
       aes(x = k, y = num_overlap, col = sublist)) +
  geom_jitter(aes(col = sublist),
              alpha = .1) +
  geom_boxplot(aes(group = interaction(k, sublist))) +
  labs(y = "Overlap") +
  theme(legend.position = "bottom")
```

Linear mixed model
```{r}
library(lme4)
library(lmerTest)
library(broom)
library(broom.mixed)
cv_res_sum_lmer <- cv_res_sum |> mutate(k = as_factor(k),
                                        k = k |> fct_shift(-1))
m_cv <- lmer(cor ~ k + sublist : k + (1 | language),
             data = cv_res_sum_lmer)
m_cv_tidy <- m_cv |> tidy()
```






```{r}
xldf_train <- xldf |> 
  filter(language %in% train_langs)

cat_modal <- xldf_train |> 
  mutate(category = fct_recode(category, 
                               descriptive_words = "descriptive_words (adverbs)")) |> 
  select(uni_lemma, category) |> 
  table() |> 
  as.data.frame() |> 
  arrange(desc(Freq)) |> 
  group_by(uni_lemma) |> 
  slice(1)
```

Mean category sizes (get proportions for each form and mean) (rebase after excluding oddball categories)
```{r}
xldf_train <- xldf_train |> 
  mutate(category = fct_recode(category,
                               descriptive_words = "descriptive_words (adjectives)",
                               descriptive_words = "descriptive_words (adverbs)",
                               quantifiers = "articles",
                               outside = "outside_places",
                               # pronouns = "prepositional_pronouns",
                               locations = "prepositions",
                               locations = "directions"))

classic_cats <- xldf |> 
  filter(language == "English (American)") |> 
  pull(category) |> 
  unique()

all_cats <- xldf_train |> 
  filter(category %in% classic_cats) |> 
  group_by(language, category) |> 
  summarise(n = n(), .groups = "drop_last") |> 
  mutate(prop = n / sum(n)) |> 
  group_by(category) |> 
  summarise(mean_prop = mean(prop)) |> 
  mutate(mean_prop = mean_prop / sum(mean_prop))

write_csv(all_cats, "data/category_proportions.csv")




```

## HMC with category proportions

```{r}
xldf <- readRDS("data/xldf_prod_allforms.rds")
```

```{r}
uni_lemmas <- xldf |> 
  group_by(uni_lemma, category) |> 
  summarise(n = n()) |> 
  arrange(desc(n)) |> 
  slice(1)

category_proportions <- read_csv(here("cat_props.csv"))
```

