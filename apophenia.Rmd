---
title: "Apophenia"
author: "George"
date: "`r Sys.Date()`"
output: html_document
---

```{r, include=F}
require(here)
require(tidyverse)
require(glue)
require(Hmisc)

source(here("cross-ling-comparison-helpers.R"))
```


```{r setup, eval=F, include=FALSE}
# for given lang, get graph of cor bw full WS and rand indices
get_random_cors <- function(lang, form="WS", nsamp=100) {
  message(glue("Processing {lang}\r"))
  load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
  WS_tot = rowSums(d_prod, na.rm=T)
  steps = seq(25, 300, by=25)
  dat = tibble()
  for(step in steps) {
    for(i in 1:nsamp) {
      tmp = c()
      rand_idx = sample(1:ncol(d_prod), step)
      tmp = c(tmp, cor(WS_tot, rowSums(d_prod[,rand_idx], na.rm=T)))
    }
    dat = dat %>% 
      bind_rows(tibble(language = lang, 
                       N = step, 
                       mean_cor = mean(tmp)))
  }
  return(dat)
}


# now the same, but sample from English
eng_unis <- subset(xldf, language=="English (American)")
get_random_english_cors <- function(lang, form="WS", nsamp=100) {
  message(glue("Processing {lang}\r"))
  load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
  # find all Eng unis that are in lang - will randomly draw from these
  poss_unis = subset(items, is.element(uni_lemma, eng_unis$uni_lemma))
  WS_tot = rowSums(d_prod, na.rm=T)
  steps = seq(25, 300, by=25)
  dat = tibble()
  for(step in steps) {
    for(i in 1:nsamp) {
      tmp = c()
      row_idx = sample(1:nrow(poss_unis), step)
      item_idx = poss_unis[row_idx,]$item_id
      tmp = c(tmp, cor(WS_tot, rowSums(d_prod[,item_idx], na.rm=T)))
    }
    dat = dat %>% 
      bind_rows(tibble(language = lang, 
                       N = step, 
                       mean_cor = mean(tmp)))
  }
  return(dat)
}


swad_list = swad_lists[[9]]
get_random_swadesh_cors <- function(lang, form="WS", nsamp=100) {
  message(glue("Processing {lang}\r"))
  load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
  # find all Swad unis that are in lang - will randomly draw from these
  poss_unis = subset(items, is.element(uni_lemma, swad_list$uni_lemma))
  WS_tot = rowSums(d_prod, na.rm=T)
  steps = seq(25, nrow(poss_unis), by=25)
  dat = tibble()
  for(step in steps) {
    for(i in 1:nsamp) {
      tmp = c()
      row_idx = sample(1:nrow(poss_unis), step)
      item_idx = poss_unis[row_idx,]$item_id
      tmp = c(tmp, cor(WS_tot, rowSums(d_prod[,item_idx], na.rm=T)))
    }
    dat = dat %>% 
      bind_rows(tibble(language = lang, 
                       N = step, 
                       mean_cor = mean(tmp)))
  }
  return(dat)
}


get_all_random_cors <- function(languages) {
  dat <- tibble()
  for(lang in languages) {
    dat <- bind_rows(dat, get_random_cors(lang) %>% mutate(Random="Item"))
    dat <- bind_rows(dat, get_random_english_cors(lang) %>% mutate(Random="English"))
    dat <- bind_rows(dat, get_random_swadesh_cors(lang) %>% mutate(Random="Swadesh"))
  }
  return(dat)
}

dat <- get_all_random_cors(languages)
save(dat, file=here("data/apophenia.Rdata"))
```

Just pick N items, either from the target language's CDI:WS, or from English.

```{r, fig.width=8, fig.height=5}
load(here("data/apophenia.Rdata"))

dat %>% ggplot(aes(x=N, y=mean_cor, color=language)) + 
  facet_wrap(. ~ Random) + 
  geom_line() + theme_bw() 
```

```{r}
dat %>% group_by(N, Random) %>%
  summarise(mean_cor = mean(mean_cor)) %>%
  ggplot(aes(x=N, y=mean_cor, color=Random)) + 
  geom_line() + theme_bw() + ylim(.97, 1)
```

## Monte Carlo Construction of Swadesh Lists

Want to build a Swadesh list (SL) of a given length (N)?

  1) construct SL from N randomly-selected uni-lemmas (ToDo: try biasing this selection: weight by uni-lemma frequency, 1/variance of difficulty, discrimination...)
  2) evaluate correlation between SL and full CDI (per language, then average those correlations--shouldn't bias based on CDI admins)
  3) select a random item i to consider removing from SL, and a replacement uni-lemma 
  4) evaluate correlation of SL vs. SL with swapped item: if swap is better, accept with 95% probability.

Fun fact: there are $\sim10^{100}$ potential lists of length 50 (2000 choose 50).

```{r, echo=F}
# for each language, look for uni-lemmas in swad_list in xldf (may not find all of them!)
# and test correlation between swad_list subsample and full CDI.
# also test correlation of full CDI against many random samples of size length(swad_list)

# run once to resave d_prod data in 1 list (to save loading time)
resave_prod_data <- function(form="WS") {
  d_prodl = list()
  for(lang in languages) {
    load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
    d_prodl[[lang]] = d_prod
  }
  save(d_prodl, file="data/WS_prod_per_lang.Rdata")
}

run_swadesh_comparisons <- function(xldf, languages, swad_list, form='WS') {
  xx <- tibble()
  for(lang in languages) {
    #load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
    swad_l <- subset(xldf, language==lang & is.element(uni_lemma, swad_list)) 
    swad_cor = cor(rowSums(d_prodl[[lang]], na.rm=T), rowSums(d_prodl[[lang]][,swad_l$item_id], na.rm=T))
    
    xx <- xx %>% bind_rows(tibble(language = lang, sublist = "Swadesh", 
                                  run = NA, cor = swad_cor, N = nrow(swad_l)))
  }
  
  xx_sum <- xx %>% 
    group_by(language, sublist, N) %>%
    summarise(r = mean(cor)) %>%
    pivot_wider(names_from = sublist, values_from = r) %>%
    rename(`Swadesh r` = Swadesh)
  return(xx_sum)
}

# WS production fits
load(here("data/multiling_2pl_WS_prod_fits.Rdata"))
load(here("data/xling-WSprod-IRTparms.Rdata")) # 23179, 
#languages = names(coefs)


# combined d_prod data
load(here("data/WS_prod_per_lang.Rdata"))

xldf <- xldf %>% mutate(uni_lemma = ifelse(uni_lemma=="NA", NA, uni_lemma)) %>%
  mutate(d = -d) %>% # easiness -> difficulty
  filter(!is.na(d)) # removes 1 Slovak word with NA parameters (because all responses were 0)

xldf <- xldf %>% filter(!is.na(uni_lemma)) # 21824 out of 23020 have uni-lemmas defined 95%

unis <- xldf %>% filter(uni_lemma!="NA") %>%
  group_by(uni_lemma) %>% # category <- but many uni-lemmas are categorized differently across languages
  summarise(d_m=mean(d), a1_m=mean(a1), d_sd = sd(d), a1_sd = sd(a1), n=n())
# maybe should just take the highest-discrimination uni-lemma per form? (some forms have >1 of some unis)

ws_tab <- get_item_n_subject_counts(models)
languages = subset(ws_tab, N>=200)$Language
```



```{r, warning=F}
uni_lemmas <- xldf |> 
  group_by(uni_lemma, category) |> 
  summarise(n = n()) |> 
  arrange(desc(n)) |> 
  slice(1) |> select(-n) 

unis <- unis %>% left_join(uni_lemmas)

# now we'll constrain to have proportion number of items per semantic category
category_proportions <- read_csv("data/category_proportions.csv") # 26 proportions <- will round up smaller categories and round down largers ones..
# minimum of 1 per category?

# unis will have category that is modal cat across all langs

make_swadesh_list <- function(unis, n_items=100, iters=1000, 
                              save_every = 50, prev_run=NULL) {
  category_proportions <- category_proportions %>% 
    mutate(n_per_category = round(mean_prop * n_items))
  
  actual_n_items = sum(category_proportions$n_per_category) # fixme - when we figure out how to fix rounding
  
  cors <- rep(NA, iters/save_every)
  accepted_rate <- rep(NA, iters/save_every)
  swad_inds <- matrix(0, nrow=iters/save_every, ncol=actual_n_items)
  if(is.null(prev_run)) {
    # initial list needs to sample n per category
    cur_swad_inds = c()
    for(ci in 1:nrow(category_proportions)) {
      this_cat_uni <- subset(unis, category==category_proportions[ci,]$category)
      new_unis = sample(this_cat_uni$uni_lemma, 
                             category_proportions[ci,]$n_per_category, replace = F)
      cur_swad_inds = c(cur_swad_inds, which(unis$uni_lemma %in% new_unis))
    }
  } else {
    cur_swad_inds = prev_run$swad_inds[nrow(prev_run$swad_inds),] # start with last saved inds
    if(ncol(prev_run$swad_inds)!=n_items) return("Error: mismatch in n_items of passed previous result")
  }
  xx_g <- run_swadesh_comparisons(xldf, languages, unis[cur_swad_inds,]$uni_lemma, form='WS')
  accepted_count = 0 # track number of accepted swaps
  for (i in 1:iters) {
    # pick item to drop, find its category
    drop_i = sample(1:n_items, 1) 
    drop_cat = unis[cur_swad_inds[drop_i],]$category
    drop_unilemma = unis[cur_swad_inds[drop_i],]$uni_lemma
    # get item from same category
    new_unilemma = sample(subset(unis, category==drop_cat & uni_lemma!=drop_unilemma)$uni_lemma, 1)
    new_i = which(unis$uni_lemma==new_unilemma) # error fixme
    #new_i = sample(setdiff(1:nrow(unis), cur_swad_inds), 1) 
    new_swad_inds = c(cur_swad_inds[-drop_i], new_i)
    xx_g2 <- run_swadesh_comparisons(xldf, languages, unis[new_swad_inds,]$uni_lemma, form='WS') 
    l1r = mean(xx_g$`Swadesh r`) 
    l2r = mean(xx_g2$`Swadesh r`)
    # if l2 has better r than l1, use it -- otherwise chance of going with l1r
    if(runif(1) < (l2r/l1r)) {
      accepted_count = accepted_count + 1
      cur_swad_inds = new_swad_inds
      xx_g = xx_g2
    }
    if(i%%save_every==0) {
      cors[i/save_every] = max(l1r, l2r)
      swad_inds[(i/save_every),] = cur_swad_inds
      accepted_rate[i/save_every] = accepted_count / save_every
      accepted_count = 0
    }
  }
  
  if(is.null(prev_run)) {
    return(list(swad_inds=swad_inds, 
              cors=cors,
              acceptance_rate = accepted_rate))
  } else {
    return(list(swad_inds = rbind(prev_run$swad_inds, swad_inds),
                cors = c(prev_run$cors, cors),
                acceptance_rate = c(prev_run$acceptance_rate, accepted_rate)))
  }
}


make_swadesh_list_old <- function(unis, n_items=100, iters=1000, 
                              save_every = 50, prev_run=NULL) {
  cors <- rep(NA, iters/save_every)
  accepted_rate <- rep(NA, iters/save_every)
  swad_inds <- matrix(0, nrow=iters/save_every, ncol=n_items)
  if(is.null(prev_run)) {
    cur_swad_inds = sample(1:nrow(unis), n_items, replace = F) # could weight by n, 1/d_sd...
  } else {
    cur_swad_inds = prev_run$swad_inds[nrow(prev_run$swad_inds),] # start with last saved inds
    if(ncol(prev_run$swad_inds)!=n_items) return("Error: mismatch in n_items of passed previous result")
  }
  xx_g <- run_swadesh_comparisons(xldf, languages, unis[cur_swad_inds,]$uni_lemma, form='WS')
  accepted_count = 0 # track number of accepted swaps
  for (i in 1:iters) {
    # swap in 1 new item
    drop_i = sample(1:n_items, 1)
    new_i = sample(setdiff(1:nrow(unis), cur_swad_inds), 1) 
    new_swad_inds = c(cur_swad_inds[-drop_i], new_i)
    xx_g2 <- run_swadesh_comparisons(xldf, languages, unis[new_swad_inds,]$uni_lemma, form='WS') 
    l1r = mean(xx_g$`Swadesh r`) 
    l2r = mean(xx_g2$`Swadesh r`)
    # if l2 has better r than l1, use it -- otherwise chance of going with l1r
    if(runif(1) < (l2r/l1r)) {
      accepted_count = accepted_count + 1
      cur_swad_inds = new_swad_inds
      xx_g = xx_g2
    }
    if(i%%save_every==0) {
      cors[i/save_every] = max(l1r, l2r)
      swad_inds[(i/save_every),] = cur_swad_inds
      accepted_rate[i/save_every] = accepted_count / save_every
      accepted_count = 0
    }
  }
  
  if(is.null(prev_run)) {
    return(list(swad_inds=swad_inds, 
              cors=cors,
              acceptance_rate = accepted_rate))
  } else {
    return(list(swad_inds = rbind(prev_run$swad_inds, swad_inds),
                cors = c(prev_run$cors, cors),
                acceptance_rate = c(prev_run$acceptance_rate, accepted_rate)))
  }
}

```

```{r, echo=F, eval=F}
r1 <- make_swadesh_list(unis, n_items=150, iters=10000)
r2 <- make_swadesh_list(unis, n_items=150, iters=10000)
r3 <- make_swadesh_list(unis, n_items=150, iters=10000)
r4 <- make_swadesh_list(unis, n_items=150, iters=10000)

# another 30k
r1 <- make_swadesh_list(unis, n_items=150, iters=10000, prev_run=r1)
r2 <- make_swadesh_list(unis, n_items=150, iters=10000, prev_run=r2)
r3 <- make_swadesh_list(unis, n_items=150, iters=10000, prev_run=r3)
r4 <- make_swadesh_list(unis, n_items=150, iters=10000, prev_run=r4)

r5 <- make_swadesh_list(unis, n_items=150, iters=20000)
r6 <- make_swadesh_list(unis, n_items=150, iters=20000)
r7 <- make_swadesh_list(unis, n_items=150, iters=20000)
r8 <- make_swadesh_list(unis, n_items=150, iters=20000)

save(r1, r2, r3, r4, r5, r6, r7, r8,
     file=here("data/MH_Swadesh_lists_v3.Rdata"))
```

Correlations on the four chains went from an initial (random) .986, .985, .984, and .981 to .996 for all four chains after 10k iterations (and still .9962 after 30k swaps). 
On average, only 8 of the initial 100 (random) items still remain after 10k swaps (7.25 after 30k).
Surprisingly, the chains started to converge: after 10k swaps, 11 uni-lemmas were on the final list of all 4 chains; 49 uni-lemmas were on the final lists of 3 or 4 chains, and 107 uni-lemmas were on the list of 2+ chains.
After 30k iterations, 21 items were on all four chains, 52 were on 3+ chains, and 110 were on 2+ chains.


Now we have 8 chains with 50k iterations.

```{r, echo=F, warning=F}
load(here("data/MH_Swadesh_lists.Rdata"))

un1k <- sort(table(c(result$swad_inds, result2$swad_inds, 
                     result3$swad_inds, result4$swad_inds)), decr=T)
#length(which(un1k>2))
# after 1k, 10 items are on 3 of 4 lists
un10k <- sort(table(c(result1_2$swad_inds, result2_2$swad_inds, 
                      result3_2$swad_inds, result4_2$swad_inds)), decr=T)
un30k <- sort(table(c(result1_3$swad_inds, result2_3$swad_inds, 
                      result3_3$swad_inds, result4_3$swad_inds)), decr=T)
```

```{r eval=F, echo=F}
# @10k
length(which(un10k>3)) # 11 on all 4 chains
length(which(un10k>2)) # 49 on 3+ chains
length(which(un10k>1)) # 107 on 2+ chains

# @30k
length(which(un30k>3)) # 21 on all 4 chains
length(which(un30k>2)) # 52 on 3+ chains
length(which(un30k>1)) # 110 on 2+ chains
```

## Acceptance rate

```{r, echo=F, warning=F}
load(here("data/MH_Swadesh_lists_v2.Rdata"))

lasti = nrow(r1$swad_inds)

iteration = 1:lasti * 50
mean_accept_rate = rowMeans(cbind(r1$acceptance_rate, r2$acceptance_rate, 
                                  r3$acceptance_rate, r4$acceptance_rate,
                                  r5$acceptance_rate, r6$acceptance_rate,
                                  r7$acceptance_rate, r8$acceptance_rate))
plot(x=iteration, y=mean_accept_rate, pch='.')
```

The swap acceptance rate start ~0.4, but drops quickly and remains low. This means we likely are not exploring enough of the space, and should make it stay higher.

## Correlation of S-CDI with Full CDI scores

(for the 26 training languages)

```{r, echo=F}
plot(x=iteration, y=r1$cors, ylim=c(.990, 0.998), pch='.',
     ylab="S-CDI correlation with full CDI")
points(x=iteration, y=r2$cors, pch=".", col='red')
points(x=iteration, y=r3$cors, pch=".", col='blue')
points(x=iteration, y=r4$cors, pch=".", col='green')
points(x=iteration, y=r5$cors, pch=".", col='lightblue')
points(x=iteration, y=r6$cors, pch=".", col='darkgreen')
points(x=iteration, y=r7$cors, pch=".", col='darkblue')
points(x=iteration, y=r8$cors, pch=".", col='magenta')
```


## Initial items still remaining

```{r, include=F, echo=F}
get_initial_items_remaining <- function(results) {
  lastind = nrow(results$swad_inds)
  remaining <- rep(NA, lastind)
  for(i in 1:lastind) {
    remaining[i] = length(intersect(results$swad_inds[1,], results$swad_inds[i,])) 
  }
  return(remaining)
}

r1$initial_remaining <- get_initial_items_remaining(r1)
r2$initial_remaining <- get_initial_items_remaining(r2)
r3$initial_remaining <- get_initial_items_remaining(r3)
r4$initial_remaining <- get_initial_items_remaining(r4)
r5$initial_remaining <- get_initial_items_remaining(r5)
r6$initial_remaining <- get_initial_items_remaining(r6)
r7$initial_remaining <- get_initial_items_remaining(r7)
r8$initial_remaining <- get_initial_items_remaining(r8)

plot(x=iteration, y=r1$initial_remaining, ylim=c(0, 150), pch='.',
     ylab="Number of Initial Items Remaining")
points(x=iteration, y=r2$initial_remaining, pch=".", col='red')
points(x=iteration, y=r3$initial_remaining, pch=".", col='blue')
points(x=iteration, y=r4$initial_remaining, pch=".", col='green')
points(x=iteration, y=r5$initial_remaining, pch=".", col='lightblue')
points(x=iteration, y=r6$initial_remaining, pch=".", col='darkgreen')
points(x=iteration, y=r7$initial_remaining, pch=".", col='darkblue')
points(x=iteration, y=r8$initial_remaining, pch=".", col='magenta')
```

On average, 17 of the 150 original items remain after 50,000 iterations.

## Comparison of final uni-lemmas across chains

Are the chains converging? Here are the number of items appearing at each frequency. That is, 19 items appear on all 8 chains; 20 appear on 7 chains, ..., 99 items appear on only one chain.

```{r, echo=F}
uni50k <- c(r1$swad_inds[lasti,], r2$swad_inds[lasti,], r3$swad_inds[lasti,], r4$swad_inds[lasti,],
            r5$swad_inds[lasti,], r6$swad_inds[lasti,], r7$swad_inds[lasti,], r8$swad_inds[lasti,])

xc_freq <- sort(table(uni50k), decreasing = T)


freq_freq <- table(xc_freq)

#on4 <- length(which(xc_freq==4)) # 40 on all 4 (27%)
#on3 <- length(which(xc_freq==3)) # 55 
#on2 <- length(which(xc_freq==2)) # 80 
#on1 <- length(which(xc_freq==1)) # 115/600 = 19%
freq_freq
```


## Items on all 8 chains

```{r, echo=F}
#alvins <- c("sweater","sick","knife","long","morning","animal","hear","jam","and",
#            "motorcycle","camera","tear","brush (object)",
#            "before","box","above","want","shoulder","skirt")
#intersect(alvins, unis[as.numeric(names(xc_freq[which(xc_freq==4)])),]$uni_lemma)
# sweater, jam, brush (object)

unis[as.numeric(names(xc_freq[which(xc_freq==8)])),]$uni_lemma
```

## Items on 7 chains

```{r, echo=F}
unis[as.numeric(names(xc_freq[which(xc_freq==7)])),]$uni_lemma
```

## Items on 6 chains

```{r, echo=F}
unis[as.numeric(names(xc_freq[which(xc_freq==6)])),]$uni_lemma
```

## Properties of items on 7+ chains

```{r, echo=F, warning=F}
sinds = as.numeric(names(xc_freq[which(xc_freq>5)]))

swad_unis <- unis[sinds,]$uni_lemma

unis <- unis %>% mutate(Swadesh = ifelse(is.element(uni_lemma, swad_unis), T, F))

unis %>% filter(n>1) %>%
  group_by(Swadesh) %>%
  summarise(d_m = mean(d_m), d_sd = mean(d_sd), 
            n = mean(n), a1_m = mean(a1_m))

#unis %>% 
#  ggplot(aes(x=a1_m, y=d_m, color=Swadesh)) +
#  geom_point()

p1 <- unis %>% 
  ggplot(aes(x=Swadesh, y=d_m)) +
  geom_violin() + theme_bw() + ylab("Difficulty") +
  stat_summary(fun.data=mean_sdl, geom="pointrange", color="black") +
  xlab("On 75%+ of Swadesh lists")

p2 <- unis %>% 
  ggplot(aes(x=Swadesh, y=d_sd)) +
  geom_violin() + theme_bw() + ylab("sd(Difficulty)") +
  stat_summary(fun.data=mean_sdl, geom="pointrange", color="black") +
  xlab("On 75%+ of Swadesh lists")

p3 <- unis %>% 
  ggplot(aes(x=Swadesh, y=a1_m)) +
  geom_violin() + theme_bw() + ylab("Discrimination") +
  stat_summary(fun.data=mean_sdl, geom="pointrange", color="black") +
  xlab("On 75%+ of Swadesh lists")

p4 <- unis %>% filter(n<50) %>%
  ggplot(aes(x=Swadesh, y=n)) +
  geom_violin() + theme_bw() + ylab("# of WS forms") +
  stat_summary(fun.data=mean_sdl, geom="pointrange", color="black") +
  xlab("On 75%+ of Swadesh lists")


ggpubr::ggarrange(p1, p2, p3, p4, nrow=2, ncol=2)
```

The critical question seems to be: are we trying enough of the items?
The presence of some of the first items on each list seems a little worrying -- but it seems fine if these items are all consistently good items. So, of the initial items that remain, are (nearly) all of those 'good' items?

On average, 7 of the items that remain on each chain frequently appear on the final list.

```{r, eval=F, echo=F}
good_inds <- as.numeric(names(xc_freq[which(xc_freq>5)])) # 69 items

intersect(r1$swad_inds[1,], good_inds) # 7
intersect(r2$swad_inds[1,], good_inds) # 6
intersect(r3$swad_inds[1,], good_inds) # 7
intersect(r4$swad_inds[1,], good_inds) # 9
intersect(r5$swad_inds[1,], good_inds) # 8
intersect(r6$swad_inds[1,], good_inds) # 5
intersect(r7$swad_inds[1,], good_inds) # 11
intersect(r8$swad_inds[1,], good_inds) # 9
# mean(7,6,7,9,8,5,11,9)
```

## Posterior probability of selection for Swadesh-CDI

```{r}
all <- c(unlist(r1$swad_inds[50:lasti,]),
         unlist(r2$swad_inds[50:lasti,]),
         unlist(r3$swad_inds[50:lasti,]),
         unlist(r4$swad_inds[50:lasti,]),
         unlist(r5$swad_inds[50:lasti,]),
         unlist(r6$swad_inds[50:lasti,]),
         unlist(r7$swad_inds[50:lasti,]),
         unlist(r8$swad_inds[50:lasti,]))

head(sort(table(all), decreasing = T), 100)  # only 643 appear here...
#tail(sort(table(all), decreasing = T), 10) 
```

