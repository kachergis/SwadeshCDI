---
title: "Apophenia"
author: "George"
date: "2023-02-01"
output: html_document
---

```{r, include=F}
require(here)
require(tidyverse)
require(glue)
```


```{r setup, eval=F, include=FALSE}
# for given lang, get graph of cor bw full WS and rand indices
get_random_cors <- function(lang, form="WS", nsamp=100) {
  message(glue("Processing {lang}\r"))
  load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
  WS_tot = rowSums(d_prod, na.rm=T)
  steps = seq(25, 300, by=25)
  dat = tibble()
  for(step in steps) {
    for(i in 1:nsamp) {
      tmp = c()
      rand_idx = sample(1:ncol(d_prod), step)
      tmp = c(tmp, cor(WS_tot, rowSums(d_prod[,rand_idx], na.rm=T)))
    }
    dat = dat %>% 
      bind_rows(tibble(language = lang, 
                       N = step, 
                       mean_cor = mean(tmp)))
  }
  return(dat)
}


# now the same, but sample from English
eng_unis <- subset(xldf, language=="English (American)")
get_random_english_cors <- function(lang, form="WS", nsamp=100) {
  message(glue("Processing {lang}\r"))
  load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
  # find all Eng unis that are in lang - will randomly draw from these
  poss_unis = subset(items, is.element(uni_lemma, eng_unis$uni_lemma))
  WS_tot = rowSums(d_prod, na.rm=T)
  steps = seq(25, 300, by=25)
  dat = tibble()
  for(step in steps) {
    for(i in 1:nsamp) {
      tmp = c()
      row_idx = sample(1:nrow(poss_unis), step)
      item_idx = poss_unis[row_idx,]$item_id
      tmp = c(tmp, cor(WS_tot, rowSums(d_prod[,item_idx], na.rm=T)))
    }
    dat = dat %>% 
      bind_rows(tibble(language = lang, 
                       N = step, 
                       mean_cor = mean(tmp)))
  }
  return(dat)
}


swad_list = swad_lists[[9]]
get_random_swadesh_cors <- function(lang, form="WS", nsamp=100) {
  message(glue("Processing {lang}\r"))
  load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
  # find all Swad unis that are in lang - will randomly draw from these
  poss_unis = subset(items, is.element(uni_lemma, swad_list$uni_lemma))
  WS_tot = rowSums(d_prod, na.rm=T)
  steps = seq(25, nrow(poss_unis), by=25)
  dat = tibble()
  for(step in steps) {
    for(i in 1:nsamp) {
      tmp = c()
      row_idx = sample(1:nrow(poss_unis), step)
      item_idx = poss_unis[row_idx,]$item_id
      tmp = c(tmp, cor(WS_tot, rowSums(d_prod[,item_idx], na.rm=T)))
    }
    dat = dat %>% 
      bind_rows(tibble(language = lang, 
                       N = step, 
                       mean_cor = mean(tmp)))
  }
  return(dat)
}


get_all_random_cors <- function(languages) {
  dat <- tibble()
  for(lang in languages) {
    dat <- bind_rows(dat, get_random_cors(lang) %>% mutate(Random="Item"))
    dat <- bind_rows(dat, get_random_english_cors(lang) %>% mutate(Random="English"))
    dat <- bind_rows(dat, get_random_swadesh_cors(lang) %>% mutate(Random="Swadesh"))
  }
  return(dat)
}

dat <- get_all_random_cors(languages)
save(dat, file=here("data/apophenia.Rdata"))
```

Just pick N items, either from the target language's CDI:WS, or from English.

```{r, fig.width=8, fig.height=5}
load(here("data/apophenia.Rdata"))

dat %>% ggplot(aes(x=N, y=mean_cor, color=language)) + 
  facet_wrap(. ~ Random) + 
  geom_line() + theme_bw() 
```

```{r}
dat %>% group_by(N, Random) %>%
  summarise(mean_cor = mean(mean_cor)) %>%
  ggplot(aes(x=N, y=mean_cor, color=Random)) + 
  geom_line() + theme_bw() + ylim(.97, 1)
```

## Monte Carlo Construction of Swadesh Lists

Want to build a Swadesh list (SL) of a given length (N)?
1) construct SL from N randomly-selected uni-lemmas
	- try biasing this selection: weight by uni-lemma frequency, 1/variance of difficulty, discrimination (slope)...
2) evaluate correlation between SL and full CDI (per language, then average those correlations--shouldn't bias based on CDI admins)
3) select a random item i to consider removing from SL; evaluate correlation of SL sans i; 

Fun fact: there are $\sim10^{100}$ potential lists of length 50 (2000 choose 50).

```{r, echo=F}
# for each language, look for uni-lemmas in swad_list in xldf (may not find all of them!)
# and test correlation between swad_list subsample and full CDI.
# also test correlation of full CDI against many random samples of size length(swad_list)

# run once to resave d_prod data in 1 list (to save loading time)
resave_prod_data <- function(form="WS") {
  d_prodl = list()
  for(lang in languages) {
    load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
    d_prodl[[lang]] = d_prod
  }
  save(d_prodl, file="data/WS_prod_per_lang.Rdata")
}

run_swadesh_comparisons <- function(xldf, languages, swad_list, form='WS') {
  xx <- tibble()
  for(lang in languages) {
    #load(here(paste("data/",form,"/",lang,"_",form,"_data.Rdata", sep='')))
    swad_l <- subset(xldf, language==lang & is.element(uni_lemma, swad_list)) 
    swad_cor = cor(rowSums(d_prodl[[lang]], na.rm=T), rowSums(d_prodl[[lang]][,swad_l$item_id], na.rm=T))
    
    xx <- xx %>% bind_rows(tibble(language = lang, sublist = "Swadesh", 
                                  run = NA, cor = swad_cor, N = nrow(swad_l)))
  }
  
  xx_sum <- xx %>% 
    group_by(language, sublist, N) %>%
    summarise(r = mean(cor)) %>%
    pivot_wider(names_from = sublist, values_from = r) %>%
    rename(`Swadesh r` = Swadesh)
  return(xx_sum)
}
```



```{r, echo=F, warning=F}
# WS production fits
load(here("data/multiling_2pl_WS_prod_fits.Rdata"))
load(here("data/xling-WSprod-IRTparms.Rdata")) # 23179, 
#languages = names(coefs)

# combined d_prod data
load(here("data/WS_prod_per_lang.Rdata"))

xldf <- xldf %>% mutate(uni_lemma = ifelse(uni_lemma=="NA", NA, uni_lemma)) %>%
  mutate(d = -d) %>% # easiness -> difficulty
  filter(!is.na(d)) # removes 1 Slovak word with NA parameters (because all responses were 0)

xldf <- xldf %>% filter(!is.na(uni_lemma)) # 21824 out of 23020 have uni-lemmas defined 95%

unis <- xldf %>% filter(uni_lemma!="NA") %>%
  group_by(uni_lemma) %>% # category <- but many uni-lemmas are categorized differently across languages
  summarise(d_m=mean(d), a1_m=mean(a1), d_sd = sd(d), a1_sd = sd(a1), n=n())
# maybe should just take the highest-discrimination uni-lemma per form? (some forms have >1 of some unis)

ws_tab <- get_item_n_subject_counts(models)
languages = subset(ws_tab, N>=200)$Language

make_swadesh_list <- function(unis, n_items=100, iters=1000, cur_list=c()) {
  cors <- rep(NA, iters)
  if(length(cur_list)==0) {
    swad_inds = sample(1:nrow(unis), n_items, replace = F) # could weight by n, 1/d_sd...
  } else {
    swad_inds = cur_list
  }
  start_list = swad_inds
  xx_g <- run_swadesh_comparisons(xldf, languages, unis[swad_inds,]$uni_lemma, form='WS')
  for (i in 1:iters) {
    # swap in 1 new item
    drop_i = sample(1:n_items, 1)
    new_i = sample(setdiff(1:nrow(unis), swad_inds), 1) 
    swad_inds2 = c(swad_inds[-drop_i], new_i)
    xx_g2 <- run_swadesh_comparisons(xldf, languages, unis[swad_inds2,]$uni_lemma, form='WS') 
    l1r = mean(xx_g$`Swadesh r`) 
    l2r = mean(xx_g2$`Swadesh r`)
    # if l2 has better r than l1, use it (5% chance of keeping l1 even if worse)
    if(l1r < l2r && runif(1)<=.95) {
      swad_inds = swad_inds2
      xx_g = xx_g2
    }
    cors[i] = max(l1r, l2r)
  }
  return(list(start_list = start_list, 
              swad_inds=swad_inds, 
              cors=cors))
}

result <- make_swadesh_list(unis, n_items=100, iters=1000)
result2 <- make_swadesh_list(unis, n_items=100, iters=1000)
result3 <- make_swadesh_list(unis, n_items=100, iters=1000)

# do more iterations:
result1_2 <- make_swadesh_list(unis, n_items=100, iters=4000, cur_list=result$swad_inds)
result2_2 <- make_swadesh_list(unis, n_items=100, iters=4000, cur_list=result2$swad_inds)
result3_2 <- make_swadesh_list(unis, n_items=100, iters=4000, cur_list=result3$swad_inds)

# are these converging at all?
length(intersect(result$swad_inds, result2$swad_inds)) # 1000: 1
length(intersect(result2$swad_inds, result3$swad_inds)) # 1000: 6
length(intersect(result$swad_inds, result3$swad_inds)) # 1000: 7

length(intersect(result1_2$swad_inds, result2_2$swad_inds)) # 5k: 
length(intersect(result2_2$swad_inds, result3_2$swad_inds)) # 5k: 
length(intersect(result1_2$swad_inds, result3_2$swad_inds)) # 5k: 
```

