---
title: "Swadesh CDI pipeline"
format: html
---

```{r}
library(wordbankr)
library(tidyverse)
library(glue)
library(mirt)
library(RColorBrewer)
require(gplots)
walk(list.files("scripts", pattern = "*.R$", full.names = TRUE), source)

S_LEN = 100 # Swadesh list length
N_RAND = 100 # number of random sublists to compare to
```

# Data fetching and stitching
Get new form definitions with unified items
```{r}
languages <- list.files("data/new_items") |> str_sub(end = -5)
pronouns <- read_csv("data/pronouns.csv") |> 
  mutate(uid = glue("{form}_{itemID}"))

rep_items <- list()

# Validate to ensure no repeated items
for (language in languages) {
  ni <- get_new_items(language, pronouns)
  forms <- ni$forms
  new_items <- ni$new_items
  rep_ids <- new_items |> 
    select(category, definition) |> 
    duplicated()
  repeated <- new_items[rep_ids,]
  
  if (nrow(repeated) > 0) {
    repeated <- repeated |> 
      mutate(language = language)
    rep_items <- c(rep_items, list(repeated))
  }
}
```

Stitch data together and save outputs
```{r, eval=F}
for (language in languages) {
  ni <- get_new_items(language, pronouns)
  df <- make_data(language, ni$forms, ni$new_items)
  
  output <- list(all_demo = df$all_demo,
                 items = ni$new_items,
                 all_long = df$all_long,
                 all_prod = df$all_prod)
  
  saveRDS(output, glue("data/all_forms/{language}_data.rds"))
}
```

# IRT modelling and parameter extraction
Run 2PL models
```{r}
languages <- list.files("data/all_forms") |> str_sub(end = -10)
```

```{r eval=F}
completed <- list.files("data/prod_models") |> str_sub(end = -28)

mirtCluster()
for (language in setdiff(languages, completed)) {
  run_2PL_model(language)
}
```

Collapse cross-linguistic item parameters
```{r eval=F}
xldf <- list()
full_fscores <- list()
n_subj <- c()
n_item <- c()
n_iter <- c()

for (language in languages) {
  lang_data <- readRDS(glue("data/all_forms/{language}_data.rds"))
  fitted <- readRDS(glue("data/prod_models/{language}_2PL_allforms_prod_fits.rds"))
  n_subj <- c(n_subj, nrow(fitted$model@Data$data))
  n_item <- c(n_item, ncol(fitted$model@Data$data))
  n_iter <- c(n_iter, fitted$model@OptimInfo$iter)
    
  lang_fscores <- data.frame(fscores(fitted$model, 
                                     method = "MAP",
                                     full.scores = F)) 
  full_fscores[[language]] <- tibble(full_theta = lang_fscores$G,
                                     full_sumscore = rowSums(lang_fscores |> select(-G, -SE_G), na.rm=T))
  # could also pull age from lang_data$all_demo
  df <- fitted$coefs |> 
    rename("uid" = "definition") |> 
    left_join(lang_data$items |> 
                select(uid, category, definition, gloss, uni_lemma) |> 
                mutate(uid = str_replace(uid, " ", ".")),
              by = "uid") |> 
    mutate(language = language)
  xldf <- c(xldf, list(df))
}
xldf <- bind_rows(xldf)
saveRDS(xldf, "data/xldf_prod_allforms.rds")
saveRDS(full_fscores, "data/full_fscores_allforms.rds")
# xx <- bind_rows(full_fscores)
# plot(xx$full_theta, xx$full_sumscore) # cor = .71

model_stats <- tibble(language = languages,
                      participants = n_subj,
                      items = n_item,
                      iterations = n_iter)
saveRDS(model_stats, "data/model_stats.rds")
```

```{r}
xldf <- readRDS("data/xldf_prod_allforms.rds")
model_stats <- readRDS("data/model_stats.rds")
```

Clean xldf
```{r}
xldf_clean <- xldf |> 
  filter(!is.na(uni_lemma), !is.na(d)) |> 
  mutate(category = case_when(
    category == "descriptive_words (adjectives)" ~ "descriptive_words",
    category == "outside_places" ~ "outside",
    .default = category))

prod_pars <- xldf_clean |> 
  arrange(language, uni_lemma, desc(a1)) |> # get most discriminating uni_lemma per lang
  select(uni_lemma, language, uid, category, language, d) |>
  group_by(uni_lemma, language) |>
  slice(1) |> 
  ungroup()

gen_langs <- model_stats |> filter(participants < 300) |> pull(language)

train_langs <- setdiff(languages, gen_langs)
```

# Cross-validation
```{r, eval=F}
cv_res <- lapply(train_langs, \(lang) {
  message(glue("Calculating for {lang}..."))
  
  prod_sum <- prod_pars |> 
    filter(language %in% train_langs,
           language != lang) |> 
    group_by(uni_lemma) |> 
    summarise(num_langs = n(),
              mean_d = mean(d, na.rm = TRUE),
              sd_d = sd(d, na.rm = TRUE))
  prod_test <- prod_pars |> 
    filter(language == lang)
  
  # calculate full fscores once (rather than once per comparison)
  full_model <- readRDS(glue("data/prod_models/{lang}_2PL_allforms_prod_fits.rds"))
  full_fscores <- fscores(full_model$model, 
                          response.pattern = full_model$model@Data$data, 
                          method = "MAP")[,1] 
  
  # Swadesh 
  prod_cors <- sapply(2:(length(train_langs)-1), \(k) {
    sublist <- make_swadesh_sublist(prod_sum, S_LEN, k)
    if (k == 23) {
      fscore_cor <- get_fscore_cor(sublist, xldf_clean, lang, full_fscores)
    } else {
      fscore_cor <- NA
    }
    c(get_difficulty_cor(sublist, prod_test), fscore_cor)
  }) |> t() |> 
    `colnames<-`(c("num_overlap", "difficulty_cor", "fscore_cor")) |> 
    as_tibble() |> 
    mutate(run = NA,
           k = 2:(length(train_langs)-1),
           language = lang,
           sublist = "Swadesh")
  
  # random
  rand_cors <- lapply(2:(length(train_langs)-1), \(k) {
    rand_cors <- sapply(1:N_RAND, \(comp) {
      sublist <- make_random_sublist(prod_sum, S_LEN, k)
      if (k == 23) {
        fscore_cor <- get_fscore_cor(sublist, xldf_clean, lang, full_fscores)
      } else {
        fscore_cor <- NA
      }
      c(get_difficulty_cor(sublist, prod_test), fscore_cor)
    }) |> t() |> 
      `colnames<-`(c("num_overlap", "difficulty_cor", "fscore_cor")) |> 
      as_tibble() |> 
      mutate(run = 1:N_RAND,
             k = k)
  }) |> 
    bind_rows() |> 
    mutate(language = lang,
           sublist = "Random")
  
  bind_rows(prod_cors, rand_cors)
}) |> bind_rows()

cv_res_sum <- cv_res |> 
  group_by(k, language, sublist) |> 
  summarise(num_overlap = mean(num_overlap),
            difficulty_cor = mean(difficulty_cor),
            fscore_cor = mean(fscore_cor)) |> 
  mutate(difficulty_cor_t = atanh(difficulty_cor), # Fisher transform
         fscore_cor_t = atanh(fscore_cor)) 

saveRDS(cv_res_sum, "data/full_vs_swadesh_cv_fscores_allforms.rds")
```

```{r}
cv_res_sum <- readRDS("data/full_vs_swadesh_cv_fscores_allforms.rds")

cv_output <- cv_res_sum |> 
  group_by(k, sublist) |> 
  summarise(num_overlap = mean(num_overlap, na.rm = TRUE),
            difficulty_cor = mean(difficulty_cor, na.rm = TRUE))
```

```{r}
ggplot(cv_res_sum,
       aes(x = k, y = difficulty_cor, col = sublist)) +
  geom_jitter(aes(col = sublist),
              alpha = .1) +
  geom_boxplot(aes(group = interaction(k, sublist))) +
  labs(y = "Difficulty correlation") +
  theme(legend.position = "bottom") + theme_classic()
```

```{r}
ggplot(cv_res_sum,
       aes(x = k, y = num_overlap, col = sublist)) +
  geom_jitter(aes(col = sublist),
              alpha = .1) +
  geom_boxplot(aes(group = interaction(k, sublist))) +
  labs(y = "Overlap") +
  theme_classic() + 
  theme(legend.position = "bottom") 
```

```{r, fig.width=7, fig.height=4}

t.test(subset(cv_res_sum, sublist=="Swadesh" & k==23)$fscore_cor - 
       subset(cv_res_sum, sublist=="Random" & k==23)$fscore_cor) # n.s.

cv_res_sum %>% filter(k==23) %>%
  ggplot(aes(x = language, y = fscore_cor, group = sublist, col = sublist)) +
  geom_point(alpha=.7) +
  labs(y = "Fscore correlation") +
  theme(legend.position = "bottom") + theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust=0.95, vjust=0.9)) 
```


CV results show that $k=23$ is the optimal value; we use this to construct the final list for generalization testing.

# Generalization test
```{r}
all_prod <- readRDS("data/allforms_prod_per_lang.rds")
```

```{r}
K = 23

prod_sum <- prod_pars |> 
  filter(language %in% train_langs) |> 
  group_by(uni_lemma) |> 
  summarise(num_langs = n(),
            mean_d = mean(d, na.rm = TRUE),
            sd_d = sd(d, na.rm = TRUE))

# Swadesh 
swadesh_sublist <- make_swadesh_sublist(prod_sum, S_LEN, K)
prod_cors <- sapply(gen_langs, \(lang) {
  get_sumscore_cor(swadesh_sublist, xldf, all_prod, lang)
}) |> t() |>
  `colnames<-`(c("num_overlap", "sumscore_cor")) |> 
  as_tibble(rownames = "language") |> 
  mutate(sublist = "Swadesh")

# random
rand_cors <- lapply(gen_langs, \(lang) {
  rand_cors <- sapply(1:N_RAND, \(comp) {
    sublist <- make_random_sublist(prod_sum, S_LEN, K)
    get_sumscore_cor(sublist, xldf, all_prod, lang)
  }) |> t() |> 
    `colnames<-`(c("num_overlap", "sumscore_cor")) |> 
    as_tibble() |> 
    mutate(run = 1:N_RAND,
           language = lang)
}) |> 
  bind_rows() |> 
  mutate(sublist = "Random")

gen_res <- bind_rows(prod_cors, rand_cors)

gen_res_sum <- gen_res |> 
  group_by(language, sublist) |> 
  summarise(num_overlap = mean(num_overlap),
            sumscore_cor = mean(sumscore_cor)) |> 
  mutate(sumscore_cor_t = atanh(sumscore_cor)) # Fisher transform
```

```{r, fig.width=6, fig.height=4}
ggplot(gen_res_sum,
       aes(x = language, y = sumscore_cor, fill = sublist)) +
  geom_col(position = "dodge") +
  labs(y = "Sumscore correlation") +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) 
```

```{r, fig.width=6, fig.height=4}
ggplot(gen_res_sum,
       aes(x = language, y = num_overlap, fill = sublist)) +
  geom_col(position = "dodge") +
  labs(y = "Overlap size") +
  theme_classic() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```


### Compare Swadesh thetas to full CDI thetas/sumscores (and to random sumscores)
```{r, eval=F}
full_fscores <- readRDS("data/full_fscores_allforms.rds")

# either do this in cross-validation (show that thetas from swadesh lists are better for training languages)
# ..may not make sense for gen languages because those models are trained on little data (may not be stable parms)

# sublist vs sumscore may be similar, but backtracked thetas should be more sensitive

# swadesh_sublist$uni_lemma -> xldf$uid

# we actually want the average difficulty for Swadesh items, and we will use this to define a model to get predicted fscores from 
#swadesh_sublist$mean_d

#  https://groups.google.com/g/mirt-package/c/TLv1JFq2tCg
  
theta_comps <- tibble()
for (lang in languages) {
  lang_data <- readRDS(glue("data/all_forms/{lang}_data.rds"))
  fitted <- readRDS(glue("data/prod_models/{lang}_2PL_allforms_prod_fits.rds"))
  # get UIDs of non-Swadesh items
  swad_item_ids <- xldf %>% filter(language==lang, is.element(uni_lemma, swadesh_sublist$uni_lemma)) %>% 
    pull(uid)
  #non_swad_item_ids <- setdiff(colnames(fitted$model@Data$data), swad_item_ids)
  # mask (NA) the responses to non-Swadesh items
  masked_resps <- fitted$model@Data$data
  masked_resps[,non_swad_item_ids] = NA
  swad_resp <- masked_resps[,swad_item_ids]
  full_sumscores <- rowSums(fitted$model@Data$data, na.rm=T)
  # lang_data$all_prod # these data still have 0-producing children -- should we infer thetas for them?
  
  full_fscores_lang <- fscores(fitted$model, response.pattern=fitted$model@Data$data, method="MAP")[,1] 
  
  # this uses the trained model for this lang -- but we want to use average Swadesh item difficulties (from training langs)
  #swad_fscores <- fscores(fitted$model, response.pattern=masked_resps, method="MAP")[,1] 
  # default method is EAP; prefer MAP or ML?
  
  # this lang Swadesh uid and uni_lemmas
  lang_swad_items <- xldf_clean |> filter(language==lang, is.element(uni_lemma, swadesh_sublist$uni_lemma))
  
  # original tis lang IRT parms

  modpars_orig <- mod2values(fitted$model)
  modpars <- modpars_orig %>% filter(is.element(item, lang_swad_items$uid))
    #mutate(value = ifelse(name=='a1', 1, value)) 
  
  new_vals <- modpars %>% filter(is.element(item, lang_swad_items$uid)) %>%
    select(item, name, value) %>%
    pivot_wider(names_from=name, values_from = value) %>%
    mutate(a1 = 1) %>% #select(-d) %>% 
    left_join(lang_swad_items %>% select(uid, uni_lemma), by=c("item"="uid")) %>% # get uni_lemma
    left_join(swadesh_sublist %>% select(uni_lemma, mean_d)) %>% # get mean Swadesh difficulty
    rename(d = mean_d) %>%
    pivot_longer(cols = c(a1,d,g,u)) %>% 
    select(-uni_lemma)
  
  # https://groups.google.com/g/mirt-package/c/TLv1JFq2tCg

  new_modpars <- modpars_orig %>% 
    left_join(new_vals, by = c("item", "name")) %>% 
    mutate(value = coalesce(value.y, value.x)) |> 
    relocate(value, .after=parnum) %>%
    mutate(est = FALSE) |> 
    select(-value.x, -value.y)
  
  class(new_modpars$item) <- "character"
  
  swad_mod <- mirt(fitted$model@Data$data, 1, pars=new_modpars)
  
  swad_fscores <- fscores(swad_mod, response.pattern=masked_resps, method="MAP")[,1] 
  
  
  

  
  # this works, but we need to 1) remove (or NA?) non-Swadesh columns, and 2) remove (or NA?) non-Swadesh parameters
  swad_mod <- mirt(fitted$model@Data$data, 1, pars=mod2values(fitted$model))
  
  # need to only modify the parameters when making the new model -- then can use fscores on the masked response patterns
  swad_mod <- mirt(fitted$model@Data$data, 1, pars=new_modpars)
  
  # Error: Rows in supplied and starting value data.frame objects do not match. Were the
  #           data or itemtype input arguments modified?
  
  swad_mod <- mirt(masked_resps[,unique(new_vals$item)], 1, pars=mod2values(fitted$model))
  # Error: Rows in supplied and starting value data.frame objects do not match. Were the
  #           data or itemtype input arguments modified?
  # parnum has a parameter index that is related to the number of participants (rows) in the data...
  # and maybe based on alphabetical order of participant and item names??
  # modpars$parnum # 1-116; 125-128... skips around!
  # can we just renumber parnum ??
  #new_modpars2 <- new_modpars %>% mutate(parnum = 1:nrow(new_modpars)) # did not work
  
  tmp <- na.omit(data.frame(cbind(full_fscores_lang, swad_fscores, full_sumscores)))
  full_theta_vs_swad_theta = cor(tmp$full_fscores_lang, tmp$swad_fscores)
  full_sum_vs_swad_theta = cor(tmp$full_sumscores, tmp$swad_fscores)
  full_sum_vs_full_theta = cor(tmp$full_sumscores, tmp$full_fscores_lang)
  
  # random theta cors
  rand_theta_cors <- sapply(1:N_RAND, \(comp) {
      sublist <- make_random_sublist(prod_sum, S_LEN, K) # use optimal K (20)
      rand_ids <- xldf %>% filter(language==lang, is.element(uni_lemma, sublist$uni_lemma)) %>% 
        pull(uid)
      
      # sumscores for random sublist:
      rand_sumscores <- rowSums(fitted$model@Data$data[,rand_ids], na.rm=T)
      
      # fscores for random sublist:
      unselected_item_ids <- xldf %>% filter(language==lang, !is.element(uni_lemma, sublist$uni_lemma)) %>% 
        pull(uid)
      # mask (NA) the responses to non-Swadesh items
      masked_resps <- fitted$model@Data$data
      masked_resps[,unselected_item_ids] = NA
      rand_fscores <- fscores(fitted$model, response.pattern=masked_resps)[,1] # ToDo: rerun with method="MAP" (slower than EAP?)
      tmp <- na.omit(data.frame(cbind(full_fscores_lang, swad_fscores, full_sumscores, 
                                      rand_fscores, rand_sumscores)))
      list(full_theta_vs_rand_sum = cor(tmp$rand_sumscores, tmp$full_fscores_lang),
           full_theta_vs_rand_theta = cor(tmp$rand_fscores, tmp$full_fscores_lang))
  }) 
  
  
  full_theta_vs_rand_sum = mean(unlist(rand_theta_cors["full_theta_vs_rand_sum",]))
  full_theta_vs_rand_theta = mean(unlist(rand_theta_cors["full_theta_vs_rand_theta",])) 
  # ToDo: save variance?
  
  theta_comps <- bind_rows(theta_comps, tibble(
    full_theta_vs_swad_theta=full_theta_vs_swad_theta,
    full_sum_vs_swad_theta=full_sum_vs_swad_theta,
    full_sum_vs_full_theta=full_sum_vs_full_theta,
    full_theta_vs_rand_sum=full_theta_vs_rand_sum,
    full_theta_vs_rand_theta=full_theta_vs_rand_theta 
  )) # full_sum_vs_rand_theta ?
  
  theta_comps
}


saveRDS(theta_comps, "data/full_vs_swadesh_fscores_allforms.rds")
```

(A few languages have fscore estimates fail to converge, resulting in mismatched vector lengths..need to label data rows with participant IDs and match up..)

```{r}
theta_comps <- readRDS("data/full_vs_swadesh_fscores_allforms.rds")

theta_comps |> arrange(desc(full_sum_vs_swad_theta)) |> kableExtra::kable(digits=3) 
```

Fscore T-tests:

```{r}
t.test(atanh(theta_comps$full_theta_vs_swad_theta), atanh(theta_comps$full_theta_vs_rand_theta))

t.test(atanh(theta_comps$full_sum_vs_swad_theta), atanh(theta_comps$full_sum_vs_full_theta))
```

Not significantly different (hooray / awww).


T-tests:

Swadesh is not significantly different than random
```{r}
t.test(gen_res_sum |> filter(sublist == "Swadesh") |> pull(sumscore_cor),
       gen_res_sum |> filter(sublist == "Random") |> pull(sumscore_cor),
       paired = TRUE)
```

```{r}
t.test(gen_res_sum |> filter(sublist == "Swadesh") |> pull(num_overlap),
       gen_res_sum |> filter(sublist == "Random") |> pull(num_overlap),
       paired = TRUE)
```

Swadesh is significantly easier than all items
```{r}
t.test(swadesh_sublist$mean_d,
       prod_sum$mean_d)
```


## Cross-linguistic similarities 

We look at the Spearman correlation between the item difficulty of each language compared to each other language. First, we look at similarities across all IRT parameters, and then we focus in on the Swadesh candidates.

```{r, echo=F, fig.width=8, fig.height=8}
get_xling_difficulty_similarity <- function(xldf) {
  dif_cors <- matrix(0, nrow=length(languages), ncol=length(languages))
  dif_sims <- tibble()
  colnames(dif_cors) = languages
  rownames(dif_cors) = languages

  for(l1 in languages) {
    for(l2 in languages) {
      tmp <- xldf %>% filter(language==l1 | language==l2, !is.na(d)) %>%
        select(uni_lemma, category, category, language, d) %>% # uid, 
        group_by(uni_lemma, language) %>%
        slice(1) %>% 
        pivot_wider(names_from = language, values_from = d) %>%
        drop_na()
      dif_cors[l1,l2] <- cor(tmp[,l1], tmp[,l2], method="spearman")
      dif_sims <- bind_rows(dif_sims, tibble("Lang1" = l1, "Lang2" = l2, 
                                             "r" = cor(tmp[,l1], tmp[,l2], method="spearman")[[1]], 
                                             "N" = nrow(tmp)))
    }
  }
  return(dif_cors)
}

dif_cors <- get_xling_difficulty_similarity(xldf_clean)

Colors = brewer.pal(11,"Spectral")
diag(dif_cors) = NA
#bad_lang = c("Mandarin (Taiwanese)")
heatmap.2(dif_cors, col=Colors)
```

### Similarity in Swadesh item parameters

```{r, echo=F, fig.width=8, fig.height=8}
swad_dif_cors <- get_xling_difficulty_similarity(xldf_clean %>% 
                                              filter(is.element(uni_lemma, swadesh_sublist$uni_lemma)))

heatmap.2(swad_dif_cors, col=Colors)
```

## Baseline language similarity data

From [Bella, Batsuren, and Giunchiglia (2021)](http://ukc.disi.unitn.it/index.php/lexsim/).

```{r}
# citation:
# Gábor Bella, Khuyagbaatar Batsuren, and Fausto Giunchiglia. (2021). A Database and Visualization of the Similarity of Contemporary Lexicons. 24th International Conference on Text, Speech, and Dialogue. Olomouc, Czech Republic. Retrieved from http://ukc.disi.unitn.it/index.php/lexsim/ January 18, 2024.
lang_sims <- read.csv(file="data/similarities_1.0.tsv", sep='\t') 
# Similarity is a value between 0 and 100, confidence (Robustness) can be Low, Medium, or High. 
# The confidence of a result depends on the sizes of the lexicons over which the similarity value was computed: the smaller the lexicon sizes, the lower the confidence.

sort(unique(lang_sims$LangName_1))

matches = intersect(unique(lang_sims$LangName_1), languages) # 16
names(matches) = matches
setdiff(languages, unique(lang_sims$LangName_1))

# xldf has: Mandarin (Beijing), Mandarin (Taiwanese) -> lang_sims has "Chinese"
# xldf: Cantonese -> "Yue Chinese"
# xldf: Norwegian -> "Norwegian Bokmål"
# xldf: "Latvian" they have "Latgalian" (historical Latvian??)
# xldf: "French (French)", "French (Quebecois)" -> "French"
# xldf: "English (American)", "English (Australian)", "English (British)", "English (Irish)", they have "English"
# xldf: "Portuguese (European)" -> "Portuguese" 
# xldf: "Spanish (Argentinian)","Spanish (Chilean)","Spanish (European)","Spanish (Mexican)","Spanish (Peruvian)" -> "Spanish"

# they don't have: "American Sign Language","British Sign Language","Kiswahili" (or Swahili), "Kigiriama", Greek (Cypriot/not),
language_mapping <- c(matches, c(
  "Mandarin (Beijing)" = "Chinese", "Mandarin (Taiwanese)" = "Chinese",
  "Cantonese" = "Yue Chinese",
  "Norwegian" = "Norwegian Bokmål",
  "Latvian" = "Latgalian", # assuming historical Latvian is close enough..
  "French (French)" = "French", "French (Quebecois)" = "French",
  "English (American)" = "English", "English (Australian)" = "English",
  "English (British)" = "English", "English (Irish)" = "English",
  "Portuguese (European)" = "Portuguese",
  "Spanish (Argentinian)" = "Spanish", "Spanish (Chilean)" = "Spanish",
  "Spanish (European)" = "Spanish", "Spanish (Mexican)" = "Spanish",
  "Spanish (Peruvian)" = "Spanish"
))
xldf <- xldf %>%
  mutate(lang_mapped = language_mapping[language])

# Replace NA values if the language doesn't have a mapping
#xldf$lang_mapped[is.na(xldf$lang_mapped)] <- xldf$language[is.na(xldf$lang_mapped)]

lang_sims <- lang_sims %>%
  filter(is.element(LangName_1, language_mapping) & 
         is.element(LangName_2, language_mapping))

# table(lang_sims$Robustness) # mostly high confidence, a few medium

# compare dendrograms: https://cran.r-project.org/web/packages/dendextend/vignettes/dendextend.html
require(dendextend)
```


